"use client"

import type React from "react"

import { useState, useRef, useEffect, useCallback } from "react"
import { Upload, Mic, Square, Play, Pause, Save, Trash2, PlusCircle, X, Maximize2, Minimize2, ChevronDown, ChevronUp } from "lucide-react"
import { Button } from "@/components/ui/button"
import { Card, CardContent, CardFooter } from "@/components/ui/card"
import { Tabs, TabsContent, TabsList, TabsTrigger } from "@/components/ui/tabs"
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from "@/components/ui/select"
import { transcribeAudio, summarizeText, analyzeText } from "@/lib/api-client"
import { toast } from "sonner"
import { useDatabase } from '@/hooks/useDatabase'
import {
  AlertDialog,
  AlertDialogAction,
  AlertDialogCancel,
  AlertDialogContent,
  AlertDialogDescription,
  AlertDialogFooter,
  AlertDialogHeader,
  AlertDialogTitle,
} from "@/components/ui/alert-dialog"
import {
  Dialog,
  DialogContent,
  DialogHeader,
  DialogTitle,
  DialogClose,
} from "@/components/ui/dialog"

interface AudioRecorderProps {
  isAuthenticated: boolean;
  onResultsChange?: (results: Array<{ id: number; type: string; content: string; title?: string; generating: boolean; date?: string; originalId?: string }>) => void;
  initialResults?: Array<{ id: number; type: string; content: string; title?: string; generating: boolean; date?: string; originalId?: string }>;
}

export default function AudioRecorder({ isAuthenticated = false, onResultsChange, initialResults = [] }: AudioRecorderProps) {
  // Basic state
  const [isRecording, setIsRecording] = useState(false)
  const [audioURL, setAudioURL] = useState<string | null>(null)
  const [recordingTime, setRecordingTime] = useState(0)
  const [isPlaying, setIsPlaying] = useState(false)
  const [uploadedFile, setUploadedFile] = useState<File | null>(null)
  const [uploadedAudioURL, setUploadedAudioURL] = useState<string | null>(null)
  const [isUploadedPlaying, setIsUploadedPlaying] = useState(false)
  const [isPostRecording, setIsPostRecording] = useState(false)
  const [selectedAiAction, setSelectedAiAction] = useState<string>("summarize")
  const [aiProcessing, setAiProcessing] = useState(false)
  const [currentMimeType, setCurrentMimeType] = useState<string>("")
  const [processedResults, setProcessedResults] = useState<Array<{ id: number; type: string; content: string; title?: string; generating: boolean; expanded?: boolean; date?: string; originalId?: string }>>(initialResults);
  const [isMinimized, setIsMinimized] = useState<boolean>(false);
  const [lastTranscriptionId, setLastTranscriptionId] = useState<string | null>(null);
  
  // Delete confirmation dialog state
  const [deleteDialogOpen, setDeleteDialogOpen] = useState(false);
  const [itemToDelete, setItemToDelete] = useState<{ id: number; type: string; originalId: string | null }>({ id: 0, type: '', originalId: null });
  
  // Modal state for expanded card
  const [modalOpen, setModalOpen] = useState(false);
  const [selectedCard, setSelectedCard] = useState<{ 
    id: number; 
    type: string; 
    content: string; 
    title?: string; 
    date?: string;
    originalId?: string;
  } | null>(null);
  
  // Recording time limit in seconds based on auth status
  const recordingTimeLimit = isAuthenticated ? 600 : 300;
  
  // Add a state for the transcript
  const [transcriptContent, setTranscriptContent] = useState<string>(
    "This is a simulated transcript of your audio recording. It would contain all the spoken words detected in your recording. In a real implementation, this would be generated by a speech-to-text service.\n\nThe transcript would be formatted with paragraphs and punctuation to make it easy to read. It might also include timestamps or speaker identification depending on the service used.",
  )

  // Refs
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const audioRef = useRef<HTMLAudioElement | null>(null)
  const timerRef = useRef<NodeJS.Timeout | null>(null)
  const uploadedAudioRef = useRef<HTMLAudioElement | null>(null)
  const canvasRef = useRef<HTMLCanvasElement | null>(null)
  const animationFrameRef = useRef<number | null>(null)
  const analyzerRef = useRef<AnalyserNode | null>(null)
  const micStreamRef = useRef<MediaStream | null>(null)
  const fileInputRef = useRef<HTMLInputElement | null>(null)
  
  // Map to store transcription content for each result
  const [transcriptMap, setTranscriptMap] = useState<Record<number, string>>({});
  // Convert lastTranscriptionId to a numeric ID for use with the transcript map
  const [lastTranscriptionNumericId, setLastTranscriptionNumericId] = useState<number | null>(null);

  // Map to store original IDs for each result
  const [originalIdMap, setOriginalIdMap] = useState<Record<number, string>>({});

  // Initialize transcriptMap from initialResults
  useEffect(() => {
    if (initialResults.length > 0) {
      console.log('AudioRecorder - Initializing transcript map from initialResults');
      const newTranscriptMap: Record<number, string> = {};
      
      initialResults.forEach(result => {
        if (result.type === 'transcribe') {
          console.log(`AudioRecorder - Adding transcription to map: ID=${result.id}, Content=${result.content ? result.content.substring(0, 50) + '...' : 'No content'}`);
          newTranscriptMap[result.id] = result.content;
        }
      });
      
      console.log('AudioRecorder - New transcript map:', Object.keys(newTranscriptMap).length, 'entries');
      setTranscriptMap(newTranscriptMap);
      
      // Also update processedResults to ensure they have the content
      setProcessedResults(initialResults.map(result => ({
        ...result,
        // Ensure expanded is set to false initially
        expanded: false
      })));
    }
  }, [initialResults]);

  // Update lastTranscriptionNumericId when lastTranscriptionId changes
  useEffect(() => {
    if (lastTranscriptionId) {
      // Convert UUID to numeric ID using the same method as in AudioWrapper
      const numericId = parseInt(lastTranscriptionId.replace(/-/g, '').substring(0, 13), 16);
      console.log('AudioRecorder - Setting lastTranscriptionNumericId:', numericId);
      setLastTranscriptionNumericId(numericId);
    }
  }, [lastTranscriptionId]);

  // Function to get transcript content for a specific result
  const getTranscriptContent = (resultId: number) => {
    // First check if it's in the map
    if (transcriptMap[resultId]) {
      console.log(`AudioRecorder - Using stored transcript for result ID ${resultId}:`, 
        typeof transcriptMap[resultId] === 'string' && transcriptMap[resultId].length > 50 
          ? transcriptMap[resultId].substring(0, 50) + '...'
          : transcriptMap[resultId]
      );
      return transcriptMap[resultId];
    }
    
    // If not in map, check if it's in the processedResults
    const result = processedResults.find(r => r.id === resultId);
    if (result && result.type === 'transcribe' && result.content) {
      console.log(`AudioRecorder - Found transcript in processedResults for ID ${resultId}:`, 
        result.content.length > 50 ? result.content.substring(0, 50) + '...' : result.content
      );
      
      // Add it to the map for future use
      setTranscriptMap(prevMap => ({
        ...prevMap,
        [resultId]: result.content
      }));
      
      return result.content;
    }
    
    // If there's no content yet but we have an originalId from the result
    if (result && result.originalId) {
      console.log(`AudioRecorder - No content in map, but have originalId for ${resultId}: ${result.originalId}`);
      
      // Set a loading message in the map
      setTranscriptMap(prevMap => ({
        ...prevMap,
        [resultId]: "Loading transcript from database..."
      }));
      
      // For summaries and analyses, we need to get the transcription via the analysis record
      if (result.type === 'summarize' || result.type === 'analyze') {
        console.log(`AudioRecorder - Fetching transcript for analysis ID ${result.originalId}`);
        // Start fetching in the background
        fetchTranscriptionForAnalysis(result.originalId, resultId)
          .then(content => {
            if (content) {
              setTranscriptMap(prevMap => ({
                ...prevMap,
                [resultId]: content
              }));
            }
          });
      } else {
        // For direct transcriptions
        console.log(`AudioRecorder - Fetching transcript from DB for transcription ID ${result.originalId}`);
        // Start fetching in the background
        fetchTranscriptionFromDB(result.originalId, resultId)
          .then(content => {
            if (content) {
              setTranscriptMap(prevMap => ({
                ...prevMap,
                [resultId]: content
              }));
            }
          });
      }
      
      return "Loading transcript from database...";
    }
    
    // If we have a lastTranscriptionId but no content yet, fetch from DB
    if (lastTranscriptionId) {
      // Start fetching from DB if not already started
      console.log(`AudioRecorder - No content, but have lastTranscriptionId: ${lastTranscriptionId}`);
      
      // Set a loading message in the map
      setTranscriptMap(prevMap => ({
        ...prevMap,
        [resultId]: "Loading transcript from database..."
      }));
      
      fetchTranscriptionFromDB(lastTranscriptionId, resultId)
        .then(content => {
          if (content) {
            setTranscriptMap(prevMap => ({
              ...prevMap,
              [resultId]: content
            }));
          }
        });
      
      return "Loading transcript from database...";
    }
    
    return "No transcript available";
  };
  
  // Visualization state for the recording button
  const [audioLevel, setAudioLevel] = useState<number>(0)
  const visualizationIntervalRef = useRef<NodeJS.Timeout | null>(null)
  
  // Add the useDatabase hook
  const { 
    uploadFile, 
    createTranscription, 
    createAnalysis,
    deleteTranscription,
    deleteAnalysis,
    getTranscription,
    getAnalysis,
    isLoading: dbLoading, 
    error: databaseError
  } = useDatabase();
  
  // Effect to check if recording time has reached the limit
  useEffect(() => {
    if (isRecording && recordingTime >= recordingTimeLimit) {
      stopRecording();
      
      // Show toast notification
      if (isAuthenticated) {
        toast.warning("Recording limit reached", {
          description: "You've reached the 10-minute recording limit."
        });
      } else {
        toast.warning("Recording limit reached", {
          description: "You've reached the 5-minute recording limit. Sign in for longer recordings."
        });
      }
    }
  }, [isRecording, recordingTime, recordingTimeLimit, isAuthenticated]);
  
  // Calculate time remaining in seconds
  const timeRemainingSeconds = recordingTimeLimit - recordingTime;
  
  // Format time remaining for display
  const formatTimeRemaining = () => {
    const mins = Math.floor(timeRemainingSeconds / 60);
    const secs = timeRemainingSeconds % 60;
    return `${mins}:${secs < 10 ? "0" : ""}${secs}`;
  };
  
  // Clean up resources when component unmounts
  useEffect(() => {
    return () => {
      // Stop any active recording
      if (mediaRecorderRef.current && mediaRecorderRef.current.state === "recording") {
        try {
          mediaRecorderRef.current.stop()
        } catch (e) {
          console.error("Error stopping media recorder:", e)
        }
      }
      
      // Clear timer
      if (timerRef.current) {
        clearInterval(timerRef.current)
      }
      
      // Clear visualization interval
      if (visualizationIntervalRef.current) {
        clearInterval(visualizationIntervalRef.current)
      }
      
      // Stop microphone stream
      if (micStreamRef.current) {
        try {
          micStreamRef.current.getTracks().forEach(track => {
            try {
              track.stop()
            } catch (e) {
              console.error("Error stopping track:", e)
            }
          })
        } catch (e) {
          console.error("Error stopping stream:", e)
        }
      }
      
      // Clean up audio URLs
      if (audioURL) {
        URL.revokeObjectURL(audioURL)
      }
      
      if (uploadedAudioURL) {
        URL.revokeObjectURL(uploadedAudioURL)
      }
    }
  }, [audioURL, uploadedAudioURL])
  
  // Handle audio element events
  useEffect(() => {
    const audioElement = audioRef.current
    
    if (audioElement) {
      const handleEnded = () => {
        console.log("Audio playback ended")
        setIsPlaying(false)
        setIsUploadedPlaying(false)
      }
      
      const handleError = (e: Event) => {
        console.error("Audio element error:", e)
        setIsPlaying(false)
        setIsUploadedPlaying(false)
      }
      
      audioElement.addEventListener('ended', handleEnded)
      audioElement.addEventListener('error', handleError)
      
      return () => {
        audioElement.removeEventListener('ended', handleEnded)
        audioElement.removeEventListener('error', handleError)
      }
    }
  }, [])
  
  // Format time for display
  const formatTime = (seconds: number) => {
    const mins = Math.floor(seconds / 60)
    const secs = seconds % 60
    return `${mins}:${secs < 10 ? "0" : ""}${secs}`
  }
  
  // Start recording
  const startRecording = async () => {
    try {
      console.log("Starting recording...")
      
      // Get microphone stream
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        }
      })
      
      // Store stream reference
      micStreamRef.current = stream
      
      // Reset chunks array
      audioChunksRef.current = []
      
      // Determine the supported MIME type
      const mimeType = getSupportedMimeType();
      console.log("Using MIME type:", mimeType);
      setCurrentMimeType(mimeType || 'audio/mp4');
      
      // Create media recorder with the supported MIME type
      try {
        const options = mimeType ? { mimeType } : { mimeType: 'audio/mp4' };
        mediaRecorderRef.current = new MediaRecorder(stream, options);
        console.log("MediaRecorder created successfully");
      } catch (error) {
        console.error("Error creating MediaRecorder:", error);
        // Try again without specifying a MIME type
        try {
          console.log("Trying to create MediaRecorder without MIME type");
          mediaRecorderRef.current = new MediaRecorder(stream);
          console.log("MediaRecorder created successfully without MIME type");
          // Get the actual MIME type being used
          if (mediaRecorderRef.current.mimeType) {
            setCurrentMimeType(mediaRecorderRef.current.mimeType);
            console.log("Using browser-selected MIME type:", mediaRecorderRef.current.mimeType);
          } else {
            setCurrentMimeType('audio/mp4'); // Default fallback
          }
        } catch (fallbackError) {
          console.error("Failed to create MediaRecorder even without MIME type:", fallbackError);
          alert("Your browser doesn't support audio recording. Please try a different browser.");
          // Clean up
          if (micStreamRef.current) {
            micStreamRef.current.getTracks().forEach(track => track.stop());
          }
          return;
        }
      }
      
      // Set up event handlers
      mediaRecorderRef.current.ondataavailable = (event) => {
        console.log("Data available:", event.data?.size)
        if (event.data && event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }
      
      mediaRecorderRef.current.onstop = () => {
        console.log("MediaRecorder stopped, chunks:", audioChunksRef.current.length);
        if (audioChunksRef.current.length === 0) {
          console.warn("No audio data was recorded");
          return;
        }
        try {
          console.log("Creating audio blob with MIME type: audio/mp4");
          const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/mp4' });
          const url = URL.createObjectURL(audioBlob);
          setAudioURL(url);
          console.log("Audio blob created successfully");

          // Start automatic transcription
          setAiProcessing(true);
          (async () => {
            try {
              console.log("Calling transcribeAudio with audio blob");
              const result = await transcribeAudio(audioBlob);
              let transcript = "";
              if (result.success && result.text) {
                transcript = result.text;
                
                // Store the audio file and transcription in the database if user is authenticated
                if (isAuthenticated) {
                  try {
                    // Create a File object from the Blob
                    const file = new File([audioBlob], `recording_${new Date().toISOString()}.mp4`, { 
                      type: 'audio/mp4' 
                    });
                    
                    // Upload the file to Supabase storage
                    const fileData = await uploadFile(file);
                    
                    if (fileData) {
                      // Create a transcription record
                      const transcriptionData = await createTranscription(
                        fileData.id,
                        "Recorded Audio Transcription",
                        transcript,
                        recordingTime, // Use the recordingTime for recorded audio
                        { source: "recorder" }
                      );
                      
                      console.log("Transcription saved to database");
                      
                      // Store the transcription ID for later use with analyses
                      setLastTranscriptionId(transcriptionData?.id || null);
                      
                      // If we have a transcription ID, convert it to a numeric ID and store the transcript in the map
                      if (transcriptionData?.id) {
                        const numericId = parseInt(transcriptionData.id.replace(/-/g, '').substring(0, 13), 16);
                        console.log('Setting transcript in map for ID:', numericId);
                        setTranscriptMap(prevMap => ({
                          ...prevMap,
                          [numericId]: transcript
                        }));
                        
                        // Also store the original ID in the originalIdMap
                        setOriginalIdMap(prevMap => ({
                          ...prevMap,
                          [numericId]: transcriptionData.id
                        }));
                      }
                    }
                  } catch (dbError) {
                    console.error("Error saving to database:", dbError);
                  }
                }
              } else {
                transcript = "Error: " + (result.error || "Unknown transcription error");
              }
              setTranscriptContent(transcript);
            } catch (e) {
              console.error("Error in transcription process:", e);
              setTranscriptContent("Error: " + (e instanceof Error ? e.message : String(e)));
            } finally {
              setAiProcessing(false);
            }
          })();

        } catch (error) {
          console.error("Error creating audio blob:", error);
          // Fallback logic
          try {
            const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/mp4' });
            const url = URL.createObjectURL(audioBlob);
            setAudioURL(url);
            setCurrentMimeType('audio/mp4');
            console.log("Created audio blob with fallback MIME type: audio/mp4");
          } catch (fallbackError) {
            console.error("Failed to create audio blob even with fallback:", fallbackError);
            alert("There was an error processing your recording.");
          }
        }
      }
      
      // Start recording
      mediaRecorderRef.current.start(100) // Capture in 100ms chunks
      
      // Update UI state
      setIsRecording(true)
      setRecordingTime(0)
      setIsPostRecording(false)
      
      // Start timer
      timerRef.current = setInterval(() => {
        setRecordingTime(prev => prev + 1)
      }, 1000)
      
      // Start visualization for the recording button
      startVisualization(stream)
      
      console.log("Recording started successfully")
    } catch (error) {
      console.error("Error starting recording:", error)
    }
  }
  
  // Start visualization for the recording button
  const startVisualization = (stream: MediaStream) => {
    try {
      // Clear any existing visualization interval
      if (visualizationIntervalRef.current) {
        clearInterval(visualizationIntervalRef.current)
      }
      
      // Create audio context and analyzer
      // Define the AudioContext type that includes webkitAudioContext
      type AudioContextType = typeof AudioContext
      
      // Define a type for the window with webkitAudioContext
      interface WindowWithWebkitAudio extends Window {
        webkitAudioContext?: AudioContextType;
      }
      
      const AudioContextClass: AudioContextType = 
        window.AudioContext || (window as WindowWithWebkitAudio).webkitAudioContext || null as unknown as AudioContextType;
      
      const audioContext = new AudioContextClass()
      const analyser = audioContext.createAnalyser()
      analyser.fftSize = 256
      analyser.smoothingTimeConstant = 0.3 // Make it more responsive (lower = more responsive)
      
      // Connect microphone to analyzer
      const source = audioContext.createMediaStreamSource(stream)
      source.connect(analyser)
      
      // Create data array for frequency data
      const dataArray = new Uint8Array(analyser.frequencyBinCount)
      
      // Update visualization at regular intervals
      visualizationIntervalRef.current = setInterval(() => {
        // Get frequency data
        analyser.getByteFrequencyData(dataArray)
        
        // Calculate average level with emphasis on lower frequencies
        // which are more common in speech
        let sum = 0
        let weight = 0
        for (let i = 0; i < dataArray.length; i++) {
          // Give more weight to lower frequencies (first third of the spectrum)
          const frequencyWeight = i < dataArray.length / 3 ? 3 : 1
          sum += dataArray[i] * frequencyWeight
          weight += frequencyWeight
        }
        const avg = sum / weight
        
        // Update audio level (0-100 scale) with smoother transitions
        // Amplify the effect by multiplying the raw value
        setAudioLevel(prev => {
          // Smooth transitions by blending previous and new values
          const amplifiedLevel = Math.min(100, avg * 1.5) // Amplify by 1.5x
          return prev * 0.2 + amplifiedLevel * 0.8 // 80% new value, 20% old value for more responsiveness
        })
      }, 20) // Update 50 times per second for smoother animation
      
      // Clean up when recording stops
      return () => {
        clearInterval(visualizationIntervalRef.current!)
        audioContext.close()
      }
    } catch (error) {
      console.error("Error starting visualization:", error)
    }
  }
  
  // Get dynamic gradient position based on audio level
  const getGradientPosition = () => {
    // Map audio level (0-100) to gradient position (100%-0%)
    // When audio level is high, gradient moves up (lower percentage)
    // When audio level is low, gradient moves down (higher percentage)
    const position = 100 - audioLevel;
    return `${position}%`
  }
  
  // Get button styles for recording button
  const getRecordingButtonStyles = () => {
    // Fixed size for the button
    const size = '4rem';
    
    // Create a dynamic background with gradient that moves based on audio level
    return {
      width: size,
      height: size,
      background: `linear-gradient(to top, #ef4444 ${getGradientPosition()}, #f87171 100%)`,
      boxShadow: `0 0 ${Math.max(5, audioLevel / 5)}px rgba(239, 68, 68, 0.6)`,
      transition: 'box-shadow 0.1s ease-in-out',
    };
  }
  
  // Get button styles for stop button
  const getStopButtonStyles = () => {
    // Fixed size for the button
    const size = '4rem';
    
    // Create a dynamic background with gradient that moves based on audio level
    return {
      width: size,
      height: size,
      background: `linear-gradient(to top, #b91c1c ${getGradientPosition()}, #ef4444 100%)`,
      boxShadow: `0 0 ${Math.max(5, audioLevel / 5)}px rgba(185, 28, 28, 0.6)`,
      transition: 'box-shadow 0.1s ease-in-out',
    };
  }
  
  // Stop recording
  const stopRecording = () => {
    try {
      console.log("Stopping recording...")
      
      // Stop timer
      if (timerRef.current) {
        clearInterval(timerRef.current)
        timerRef.current = null
      }
      
      // Stop visualization
      if (visualizationIntervalRef.current) {
        clearInterval(visualizationIntervalRef.current)
        visualizationIntervalRef.current = null
      }
      
      // Stop media recorder
      if (mediaRecorderRef.current && mediaRecorderRef.current.state === "recording") {
        mediaRecorderRef.current.stop()
      }
      
      // Stop microphone stream
      if (micStreamRef.current) {
        micStreamRef.current.getTracks().forEach(track => track.stop())
        micStreamRef.current = null
      }
      
      // Update UI state
      setIsRecording(false)
      setIsPostRecording(true)
      setAudioLevel(0)
      
      console.log("Recording stopped successfully")
    } catch (error) {
      console.error("Error stopping recording:", error)
      setIsRecording(false)
      setIsPostRecording(true)
    }
  }
  
  // Reset recorder to initial state
  const resetRecorder = () => {
    // Clear previous recording
    if (audioURL) {
      URL.revokeObjectURL(audioURL)
      setAudioURL(null)
    }
    
    // Reset to initial state
    setIsPostRecording(false)
    setIsRecording(false)
    setRecordingTime(0)
    setAiProcessing(false)
    setIsPlaying(false)
    
    // Stop audio playback if it's playing
    if (audioRef.current) {
      audioRef.current.pause();
      audioRef.current.currentTime = 0;
    }
  }
  
  // Handle file upload
  const handleFileUpload = (event: React.ChangeEvent<HTMLInputElement>) => {
    const files = event.target.files
    if (files && files.length > 0) {
      const file = files[0]
      
      // Check if the file is an audio or video file (MP4 files are often identified as video/mp4)
      if (!file.type.startsWith('audio/') && !file.type.startsWith('video/')) {
        alert('Please upload an audio or video file')
        return
      }
      
      console.log("Uploaded file type:", file.type)
      setUploadedFile(file)
      
      // Create a URL for the uploaded file
      const url = URL.createObjectURL(file)
      
      // If we previously had an uploaded file, revoke its URL
      if (uploadedAudioURL) {
        URL.revokeObjectURL(uploadedAudioURL)
      }
      
      setUploadedAudioURL(url)
      setCurrentMimeType('audio/mp4') // Treat all uploads as MP4 for consistency
      
      // Reset any previous recording state
      if (isPostRecording) {
        resetRecorder()
      }
      
      // Start automatic transcription of the uploaded file
      setAiProcessing(true);
      
      // Convert File to Blob for transcription
      (async () => {
        try {
          console.log("Calling transcribeAudio with uploaded file");
          const result = await transcribeAudio(file);
          let transcript = "";
          if (result.success && result.text) {
            transcript = result.text;
            toast.success("Transcription complete", {
              description: "Your uploaded file has been transcribed successfully."
            });
            
            // Store the uploaded file and transcription in the database if user is authenticated
            if (isAuthenticated) {
              try {
                // Upload the file to Supabase storage
                const fileData = await uploadFile(file);
                
                if (fileData) {
                  // We need to set the audio source and wait for metadata to load
                  // to get the duration
                  const getDuration = () => {
                    return new Promise<number>((resolve) => {
                      if (audioRef.current) {
                        // If the audio element already has metadata loaded
                        if (audioRef.current.duration) {
                          resolve(Math.round(audioRef.current.duration));
                        } else {
                          // Wait for metadata to load
                          const handleLoadedMetadata = () => {
                            const duration = Math.round(audioRef.current!.duration);
                            audioRef.current!.removeEventListener('loadedmetadata', handleLoadedMetadata);
                            resolve(duration);
                          };
                          audioRef.current.addEventListener('loadedmetadata', handleLoadedMetadata);
                          audioRef.current.src = url;
                        }
                      } else {
                        resolve(0); // Fallback if no audio element
                      }
                    });
                  };
                  
                  const duration = await getDuration();
                  
                  // Create a transcription record
                  const transcriptionData = await createTranscription(
                    fileData.id,
                    file.name || "Uploaded Audio Transcription",
                    transcript,
                    duration || 0,
                    { source: "upload" }
                  );
                  
                  console.log("Transcription saved to database");
                  
                  // Store the transcription ID for later use with analyses
                  setLastTranscriptionId(transcriptionData?.id || null);
                  
                  // If we have a transcription ID, convert it to a numeric ID and store the transcript in the map
                  if (transcriptionData?.id) {
                    const numericId = parseInt(transcriptionData.id.replace(/-/g, '').substring(0, 13), 16);
                    console.log('Setting transcript in map for ID:', numericId);
                    setTranscriptMap(prevMap => ({
                      ...prevMap,
                      [numericId]: transcript
                    }));
                    
                    // Also store the original ID in the originalIdMap
                    setOriginalIdMap(prevMap => ({
                      ...prevMap,
                      [numericId]: transcriptionData.id
                    }));
                  }
                }
              } catch (dbError) {
                console.error("Error saving uploaded file to database:", dbError);
              }
            }
          } else {
            transcript = "Error: " + (result.error || "Unknown transcription error");
            toast.error("Transcription failed", {
              description: result.error || "Unknown transcription error"
            });
          }
          setTranscriptContent(transcript);
        } catch (e) {
          console.error("Error in transcription process:", e);
          setTranscriptContent("Error: " + (e instanceof Error ? e.message : String(e)));
          toast.error("Transcription failed", {
            description: "An error occurred during transcription."
          });
        } finally {
          setAiProcessing(false);
        }
      })();
    }
  }
  
  // Toggle play recorded audio
  const togglePlayRecorded = () => {
    if (audioRef.current && audioURL) {
      if (isPlaying) {
        audioRef.current.pause()
      } else {
        // Make sure we reset the audio element before setting a new source
        audioRef.current.pause()
        audioRef.current.currentTime = 0
        audioRef.current.src = audioURL
        
        // Add error handling for playback
        const playPromise = audioRef.current.play()
        if (playPromise !== undefined) {
          playPromise.catch(error => {
            console.error("Error playing recorded audio:", error)
            alert("Error playing audio. Please try recording again.")
            setIsPlaying(false)
          })
        }
      }
      setIsPlaying(!isPlaying)
    }
  }
  
  // Toggle play uploaded audio
  const togglePlayUploaded = () => {
    if (audioRef.current && uploadedAudioURL) {
      if (isUploadedPlaying) {
        audioRef.current.pause()
      } else {
        // Make sure we reset the audio element before setting a new source
        audioRef.current.pause()
        audioRef.current.currentTime = 0
        audioRef.current.src = uploadedAudioURL
        
        // Add error handling for playback
        const playPromise = audioRef.current.play()
        if (playPromise !== undefined) {
          playPromise.catch(error => {
            console.error("Error playing uploaded audio:", error)
            alert("Error playing audio. The file format may not be supported.")
            setIsUploadedPlaying(false)
          })
        }
      }
      setIsUploadedPlaying(!isUploadedPlaying)
    }
  }
  
  // Process with AI
  const processWithAI = async () => {
    setAiProcessing(true);
    const newId = new Date().getTime();
    
    // Only add to processedResults for summarize and analyze actions, not for transcripts
    if (selectedAiAction !== "transcribe") {
      // Add a new result card with generating state
      setProcessedResults(prev => [
        ...prev,
        { 
          id: newId, 
          type: selectedAiAction, 
          content: "", 
          title: "", 
          generating: true, 
          expanded: false,
          date: new Date().toISOString() // Add date field
        }
      ]);
    }

    try {
      if (selectedAiAction === "summarize") {
        const result = await summarizeText(transcriptContent);
        
        if (result.success) {
          const content = result.content || result.result || "";
          const title = result.title || "Summary";
          
          // Update the result card with the generated content
          setProcessedResults(prev => prev.map(item => 
            item.id === newId 
              ? { 
                  ...item, 
                  content, 
                  title, 
                  generating: false 
                } 
              : item
          ));
          
          // Store the analysis in the database if user is authenticated
          if (isAuthenticated) {
            try {
              // Use the lastTranscriptionId if available, otherwise use a placeholder
              const transcriptionId = lastTranscriptionId || "00000000-0000-0000-0000-000000000000";
              
              const analysisData = await createAnalysis(
                transcriptionId,
                title,
                content,
                "summary",
                { source: "recorder" }
              );
              
              console.log("Summary saved to database");
              
              // Add the new result to the processed results
              const newResult = {
                id: Date.now(),
                type: selectedAiAction,
                content: result.success && result.content ? result.content : "Error generating summary",
                title: result.title || "Summary",
                generating: false,
                date: new Date().toISOString(),
                originalId: analysisData?.id
              };
              
              // If we have an analysis ID, store it in the originalIdMap
              if (analysisData?.id) {
                setOriginalIdMap(prevMap => ({
                  ...prevMap,
                  [newResult.id]: analysisData.id
                }));
              }
              
              setProcessedResults(prev => [newResult, ...prev]);
            } catch (dbError) {
              console.error("Error saving summary to database:", dbError);
            }
          }
        } else {
          // Handle error
          setProcessedResults(prev => prev.map(item => 
            item.id === newId 
              ? { 
                  ...item, 
                  content: `Error: ${result.error || "Failed to generate summary"}`, 
                  title: "Error", 
                  generating: false 
                } 
              : item
          ));
        }
      } else if (selectedAiAction === "analyze") {
        const result = await analyzeText(transcriptContent);
        
        if (result.success) {
          const content = result.content || result.result || "";
          const title = result.title || "Analysis";
          
          // Update the result card with the generated content
          setProcessedResults(prev => prev.map(item => 
            item.id === newId 
              ? { 
                  ...item, 
                  content,
                  title,
                  generating: false 
                } 
              : item
          ));
          
          // Store the analysis in the database if user is authenticated
          if (isAuthenticated) {
            try {
              // Use the lastTranscriptionId if available, otherwise use a placeholder
              const transcriptionId = lastTranscriptionId || "00000000-0000-0000-0000-000000000000";
              
              const analysisData = await createAnalysis(
                transcriptionId,
                title,
                content,
                "analysis",
                { source: "recorder" }
              );
              
              console.log("Analysis saved to database");
              
              // Add the new result to the processed results
              const newResult = {
                id: Date.now(),
                type: selectedAiAction,
                content: result.success && result.content ? result.content : "Error generating analysis",
                title: result.title || "Analysis",
                generating: false,
                date: new Date().toISOString(),
                originalId: analysisData?.id
              };
              
              // If we have an analysis ID, store it in the originalIdMap
              if (analysisData?.id) {
                setOriginalIdMap(prevMap => ({
                  ...prevMap,
                  [newResult.id]: analysisData.id
                }));
              }
              
              setProcessedResults(prev => [newResult, ...prev]);
            } catch (dbError) {
              console.error("Error saving analysis to database:", dbError);
            }
          }
        } else {
          // Handle error
          setProcessedResults(prev => prev.map(item => 
            item.id === newId 
              ? { 
                  ...item, 
                  content: `Error: ${result.error || "Failed to generate analysis"}`, 
                  title: "Error", 
                  generating: false 
                } 
              : item
          ));
        }
      }
    } catch (error) {
      console.error("Error in AI processing:", error);
      
      // Update the result card with the error
      setProcessedResults(prev => prev.map(item => 
        item.id === newId 
          ? { 
              ...item, 
              content: `Error: ${error instanceof Error ? error.message : String(error)}`, 
              title: "Error", 
              generating: false 
            } 
          : item
      ));
    } finally {
      setAiProcessing(false);
    }
  };

  // Helper function to determine the supported MIME type
  const getSupportedMimeType = () => {
    // Only support MP4 format as requested
    if (MediaRecorder.isTypeSupported('audio/mp4')) {
      console.log('Browser supports recording in audio/mp4 format');
      return 'audio/mp4';
    }
    
    console.warn('MP4 format not supported. Using default browser implementation.');
    // Fallback to default (browser will choose)
    return '';
  };

  // Helper function to get file extension from MIME type
  const getFileExtensionFromMimeType = (mimeType: string): string => {
    // Default to mp4 as requested
    if (!mimeType || mimeType.includes('mp4')) return 'mp4';
    if (mimeType.includes('webm')) return 'webm';
    if (mimeType.includes('ogg')) return 'ogg';
    if (mimeType.includes('wav')) return 'wav';
    if (mimeType.includes('mp3') || mimeType.includes('mpeg')) return 'mp3';
    if (mimeType.includes('aac')) return 'aac';
    
    // Default fallback
    return 'mp4';
  };

  // Call onResultsChange when processedResults changes
  useEffect(() => {
    if (onResultsChange) {
      onResultsChange(processedResults);
    }
  }, [processedResults, onResultsChange]);

  // Function to toggle expanded state for a card
  const toggleCardExpanded = (id: number) => {
    console.log(`AudioRecorder - Toggling card expanded state for ID ${id}`);
    
    // Find the card to display in the modal
    const result = processedResults.find(r => r.id === id);
    if (result) {
      setSelectedCard(result);
      setModalOpen(true);
      
      // If it's a transcript card, ensure we have the transcript content
      if (result.type === 'transcribe') {
        console.log(`AudioRecorder - Expanding transcription card ${id}, checking for content`);
        
        // If we don't have the content in the map, try to get it from the result
        if (!transcriptMap[id] && result.content) {
          console.log(`AudioRecorder - Adding missing transcript to map for ID ${id}`);
          setTranscriptMap(prevMap => ({
            ...prevMap,
            [id]: result.content
          }));
        }
      }
    }
  };

  // Add useEffect to show database errors
  useEffect(() => {
    if (databaseError) {
      toast.error("Database Error", {
        description: databaseError
      });
    }
  }, [databaseError]);

  // Notify parent component about initial results
  useEffect(() => {
    console.log('AudioRecorder - Initial results:', initialResults.length);
    console.log('AudioRecorder - Processed results:', processedResults.length);
    
    if (initialResults.length > 0 && onResultsChange) {
      console.log('AudioRecorder - Notifying parent about initial results');
      onResultsChange(processedResults);
    }
  }, [initialResults, onResultsChange, processedResults]);

  // Set initial results when they change
  useEffect(() => {
    if (initialResults.length > 0) {
      console.log('AudioRecorder - Setting initial results:', initialResults.length);
      setProcessedResults(initialResults);
    }
  }, [initialResults]);

  // Function to handle delete confirmation
  const handleDeleteClick = (id: number, type: string, originalId: string | null) => {
    console.log(`AudioRecorder - Opening delete confirmation for ${type} ID ${id}, original ID: ${originalId}`);
    setItemToDelete({ id, type, originalId });
    setDeleteDialogOpen(true);
  };

  // Function to handle actual deletion
  const handleConfirmDelete = async () => {
    console.log(`AudioRecorder - Confirming delete for ${itemToDelete.type} ID ${itemToDelete.id}`);
    
    if (!itemToDelete.originalId) {
      console.error('Cannot delete item: missing original ID');
      toast.error('Delete failed', { description: 'Could not find the item to delete' });
      return;
    }
    
    try {
      let success = false;
      
      if (itemToDelete.type === 'transcribe') {
        success = await deleteTranscription(itemToDelete.originalId);
      } else if (itemToDelete.type === 'summarize' || itemToDelete.type === 'analyze') {
        success = await deleteAnalysis(itemToDelete.originalId);
      }
      
      if (success) {
        // Remove the item from processedResults
        setProcessedResults(prevResults => 
          prevResults.filter(result => result.id !== itemToDelete.id)
        );
        
        // Notify parent component
        if (onResultsChange) {
          onResultsChange(processedResults.filter(result => result.id !== itemToDelete.id));
        }
        
        toast.success('Item deleted successfully');
      } else {
        toast.error('Delete failed', { description: 'Could not delete the item' });
      }
    } catch (error) {
      console.error('Error deleting item:', error);
      toast.error('Delete failed', { description: String(error) });
    } finally {
      setDeleteDialogOpen(false);
    }
  };

  // Initialize originalIdMap from initialResults
  useEffect(() => {
    if (initialResults.length > 0) {
      console.log('AudioRecorder - Initializing originalIdMap from initialResults');
      const newOriginalIdMap: Record<number, string> = {};
      
      initialResults.forEach(result => {
        if (result.originalId) {
          console.log(`AudioRecorder - Adding original ID to map: ID=${result.id}, Original ID=${result.originalId}`);
          newOriginalIdMap[result.id] = result.originalId;
        }
      });
      
      console.log('AudioRecorder - New originalIdMap:', Object.keys(newOriginalIdMap).length, 'entries');
      setOriginalIdMap(newOriginalIdMap);
    }
  }, [initialResults]);

  // Function to get the original transcript from the database for a given analysis
  const fetchTranscriptionForAnalysis = useCallback(async (analysisId: string, resultId?: number) => {
    try {
      if (!analysisId) {
        console.error("Cannot fetch transcript: analysisId is undefined or null");
        return null;
      }
      
      console.log(`AudioRecorder - Attempting to fetch transcription for analysis with ID: ${analysisId}`);
      
      // First get the analysis to find its transcription_id
      const analysis = await getAnalysis(analysisId);
      
      if (!analysis) {
        console.log(`AudioRecorder - No analysis found with ID: ${analysisId}`);
        return "Analysis not found in the database.";
      }
      
      const transcriptionId = analysis.transcription_id;
      console.log(`AudioRecorder - Found transcription_id ${transcriptionId} for analysis ${analysisId}`);
      
      if (!transcriptionId) {
        console.log(`AudioRecorder - No transcription_id found for analysis ${analysisId}`);
        return "No associated transcript found for this analysis.";
      }
      
      // Now get the transcription
      const transcription = await getTranscription(transcriptionId);
      
      if (transcription) {
        console.log(`AudioRecorder - Successfully fetched transcription from DB, ID: ${transcriptionId}`);
        
        // Use the provided resultId if available, otherwise use lastTranscriptionNumericId
        const mapKey = resultId !== undefined ? resultId : (lastTranscriptionNumericId || 0);
        
        // Update transcript map with the content from the database
        setTranscriptMap(prevMap => ({
          ...prevMap,
          [mapKey]: transcription.content
        }));
        
        return transcription.content;
      } else {
        console.log(`AudioRecorder - No transcription found with ID: ${transcriptionId}`);
        return "Associated transcript not found in the database.";
      }
    } catch (error) {
      console.error("Error fetching transcription for analysis:", error);
      return "Error loading associated transcript.";
    }
  }, [getAnalysis, getTranscription, lastTranscriptionNumericId]);

  // Prefetch transcription data for analyses and summaries when component mounts
  useEffect(() => {
    if (initialResults.length > 0) {
      console.log('AudioRecorder - Checking if we need to prefetch any transcription data');
      
      // Get all analyses and summaries that have originalId
      const analysesAndSummaries = initialResults.filter(
        result => (result.type === 'analyze' || result.type === 'summarize') && result.originalId
      );
      
      if (analysesAndSummaries.length > 0) {
        console.log(`AudioRecorder - Found ${analysesAndSummaries.length} analyses/summaries that might need transcription data`);
        
        // Prefetch the first one to avoid loading delay when user first clicks
        const firstItem = analysesAndSummaries[0];
        if (firstItem.originalId) {
          console.log(`AudioRecorder - Prefetching transcription data for analysis ID ${firstItem.originalId}`);
          fetchTranscriptionForAnalysis(firstItem.originalId).catch(err => {
            console.error('Error prefetching transcription data:', err);
          });
        }
      }
    }
  }, [initialResults, fetchTranscriptionForAnalysis]);

  // Function to get the original transcript from the database for a given originalId
  const fetchTranscriptionFromDB = useCallback(async (originalId: string, resultId?: number) => {
    try {
      if (!originalId) {
        console.error("Cannot fetch transcript: originalId is undefined or null");
        return null;
      }
      
      console.log(`AudioRecorder - Attempting to fetch transcription with ID: ${originalId}`);
      
      // Get the transcription from the database
      const transcription = await getTranscription(originalId);
      
      if (transcription) {
        console.log(`AudioRecorder - Successfully fetched transcription from DB, ID: ${originalId}`);
        
        // Use the provided resultId if available, otherwise use lastTranscriptionNumericId
        const mapKey = resultId !== undefined ? resultId : (lastTranscriptionNumericId || 0);
        
        // Update transcript map with the content from the database
        setTranscriptMap(prevMap => ({
          ...prevMap,
          [mapKey]: transcription.content
        }));
        
        return transcription.content;
      } else {
        console.log(`AudioRecorder - No transcription found with ID: ${originalId}`);
        return "Transcript not found in the database.";
      }
    } catch (error) {
      console.error("Error fetching transcription:", error);
      
      // Check if it's the specific error we're seeing
      if (error instanceof Error && error.message.includes("multiple (or no) rows returned")) {
        console.log("This is likely because the transcription ID is invalid or the record doesn't exist");
        toast.error("Transcript not found", { 
          description: "The original transcript couldn't be found in the database." 
        });
        return "Original transcript not available.";
      }
      
      toast.error("Failed to load transcript", { 
        description: "Could not fetch the original transcript from the database." 
      });
      return "Error loading transcript.";
    }
  }, [getTranscription, lastTranscriptionNumericId]);

  return (
    <>
      {processedResults.length > 0 && (
        <div className="fixed top-32 left-0 right-0 px-8 z-[60] overflow-y-auto" style={{ maxHeight: 'calc(100vh - 200px)' }}>
          <div className="grid grid-cols-1 sm:grid-cols-1 md:grid-cols-2 lg:grid-cols-2 xl:grid-cols-3 gap-6 pb-32">
            {processedResults.filter(result => result.type !== "transcribe").map(result => (
              <div key={result.id}>
                <div 
                  className="w-full h-[250px] bg-white/10 backdrop-blur-md rounded-lg border border-white/20 shadow-lg overflow-hidden flex flex-col relative"
                >
                  <div className="p-3 border-b border-white/20 bg-white/5 flex items-center">
                    <Button
                      variant="ghost"
                      size="icon"
                      className="h-6 w-6 rounded-full bg-white/10 hover:bg-red-500/20 text-white/70 hover:text-red-400 flex-shrink-0 mr-2"
                      onClick={(e) => {
                        e.stopPropagation();
                        // Get the original ID from the result or from the originalIdMap
                        const originalId = result.originalId || originalIdMap[result.id] || null;
                        handleDeleteClick(result.id, result.type, originalId);
                      }}
                      disabled={result.generating}
                    >
                      <Trash2 className="h-3 w-3" />
                    </Button>
                    <h4 className="font-medium text-white text-base truncate flex-1">
                      {result.generating ? 
                        (result.type === "summarize" ? "Generating Summary..." : "Generating Analysis...") : 
                        (result.title || (result.type === "summarize" ? "Summary" : "Analysis"))}
                    </h4>
                    <Button
                      variant="ghost"
                      size="icon"
                      className="h-6 w-6 rounded-full bg-white/10 hover:bg-white/20 text-white/70 hover:text-white flex-shrink-0 ml-2"
                      onClick={() => toggleCardExpanded(result.id)}
                      disabled={result.generating}
                    >
                      <Maximize2 className="h-3 w-3" />
                    </Button>
                  </div>
                  <div className="p-3 flex-1 overflow-y-auto text-sm text-white/90">
                    {result.generating ? (
                      <div className="flex items-center justify-center h-full">
                        <div className="animate-pulse">Generating...</div>
                      </div>
                    ) : (
                      <>
                        <div className="whitespace-pre-line line-clamp-7">
                          {result.content.includes("This is a") ? 
                            result.content.replace(/^This is a (summary|analysis) of the audio recording\.\s+/, "") : 
                            result.content}
                        </div>
                        <div className="text-xs text-white/50 absolute bottom-3 left-3 pt-5">
                          <span>{new Date(result.date || Date.now()).toLocaleDateString('en-US', { 
                            month: 'short', 
                            day: 'numeric',
                            hour: '2-digit',
                            minute: '2-digit'
                          })}</span>
                        </div>
                      </>
                    )}
                  </div>
                </div>
              </div>
            ))}
          </div>
        </div>
      )}

      {/* Delete confirmation dialog */}
      <AlertDialog open={deleteDialogOpen} onOpenChange={setDeleteDialogOpen}>
        <AlertDialogContent className="bg-gray-800 border border-gray-700 text-white">
          <AlertDialogHeader>
            <AlertDialogTitle>Are you sure?</AlertDialogTitle>
            <AlertDialogDescription className="text-gray-300">
              This will permanently delete this {itemToDelete.type === 'transcribe' ? 'transcription' : 
                itemToDelete.type === 'summarize' ? 'summary' : 'analysis'}.
              This action cannot be undone.
            </AlertDialogDescription>
          </AlertDialogHeader>
          <AlertDialogFooter>
            <AlertDialogCancel className="bg-gray-700 text-white hover:bg-gray-600">Cancel</AlertDialogCancel>
            <AlertDialogAction 
              className="bg-red-600 text-white hover:bg-red-700" 
              onClick={handleConfirmDelete}
            >
              Delete
            </AlertDialogAction>
          </AlertDialogFooter>
        </AlertDialogContent>
      </AlertDialog>

      <Card className={`fixed bottom-4 left-1/2 transform -translate-x-1/2 ${isMinimized ? 'w-auto' : 'w-full max-w-md'} overflow-hidden bg-white/10 backdrop-blur-md border-0 shadow-xl z-[100] transition-all duration-300`}>
        <div className="absolute top-2 right-2 z-10">
          <Button
            variant="ghost"
            size="icon"
            className="h-6 w-6 rounded-full bg-white/10 hover:bg-white/20 text-white/70 hover:text-white"
            onClick={() => setIsMinimized(!isMinimized)}
          >
            {isMinimized ? <Maximize2 className="h-3 w-3" /> : <X className="h-3 w-3" />}
          </Button>
        </div>
        {isMinimized ? (
          <div className="p-3 flex items-center space-x-3">
            <Button
              variant="default"
              style={getRecordingButtonStyles()}
              className="rounded-full p-0 flex items-center justify-center h-10 w-10"
              onClick={() => {
                if (isRecording) {
                  stopRecording();
                } else {
                  setIsMinimized(false);
                  startRecording();
                }
              }}
            >
              {isRecording ? <Square className="h-4 w-4" /> : <Mic className="h-4 w-4" />}
            </Button>
            <span className="text-xs text-white/80 font-medium">
              {isRecording ? formatTimeRemaining() : "Record Audio"}
            </span>
          </div>
        ) : (
        <CardContent className="p-5">
          <Tabs defaultValue="record" className="w-full">
            <TabsList className="grid w-full grid-cols-2 mb-6 bg-white/10 p-1 rounded-lg">
              <TabsTrigger
                value="record"
                className="data-[state=active]:bg-white/20 data-[state=active]:text-white data-[state=active]:shadow-md text-white/70"
              >
                Record
              </TabsTrigger>
              <TabsTrigger
                value="upload"
                className="data-[state=active]:bg-white/20 data-[state=active]:text-white data-[state=active]:shadow-md text-white/70"
              >
                Upload
              </TabsTrigger>
            </TabsList>

            <TabsContent value="record" className="space-y-5">
              {/* Recording controls */}
              <div className="space-y-4">
                {/* Timer display - show time remaining when recording, or elapsed time after recording */}
                {isRecording ? (
                  <div className="text-center">
                    <div className="text-3xl font-mono font-bold text-white">
                      {formatTimeRemaining()}
                    </div>
                    <div className="text-xs text-white/70 mt-1">
                      {isAuthenticated 
                        ? "Premium: 10 minute recording limit" 
                        : "Free: 5 minute recording limit"}
                    </div>
                  </div>
                ) : isPostRecording && (
                  <div className="text-center">
                    <span className="text-3xl font-mono font-bold text-white">
                      {formatTime(recordingTime)}
                    </span>
                  </div>
                )}

                {/* Control buttons */}
                <div className="flex justify-center gap-4">
                  {isRecording ? (
                    <Button
                      variant="destructive"
                      style={getStopButtonStyles()}
                      className="rounded-full p-0 flex items-center justify-center"
                      onClick={stopRecording}
                    >
                      <Square className="h-6 w-6" />
                    </Button>
                  ) : isPostRecording ? (
                    <div className="flex gap-3">
                      <Button
                        variant="outline"
                        size="icon"
                        className="rounded-full w-12 h-12 border-white/20 bg-white/10 text-white hover:bg-white/20 hover:text-white"
                        onClick={resetRecorder}
                      >
                        <Trash2 className="h-5 w-5" />
                      </Button>
                      <Button
                        variant="outline"
                        size="icon"
                        className="rounded-full w-12 h-12 border-white/20 bg-white/10 text-white hover:bg-white/20 hover:text-white"
                        onClick={togglePlayRecorded}
                      >
                        {isPlaying ? <Pause className="h-5 w-5" /> : <Play className="h-5 w-5" />}
                      </Button>
                      <Button
                        variant="outline"
                        size="icon"
                        className="rounded-full w-12 h-12 border-white/20 bg-white/10 text-white hover:bg-white/20 hover:text-white"
                        onClick={resetRecorder}
                      >
                        <PlusCircle className="h-5 w-5" />
                      </Button>
                    </div>
                  ) : (
                    <div className="flex flex-col items-center">
                      <Button
                        variant="default"
                        style={getRecordingButtonStyles()}
                        className="rounded-full p-0 flex items-center justify-center mb-2"
                        onClick={startRecording}
                      >
                        <Mic className="h-6 w-6" />
                      </Button>
                      <span className="text-xs text-white/60">Tap to record</span>
                    </div>
                  )}
                </div>
              </div>

              {/* AI Processing section - only show after recording */}
              {isPostRecording && audioURL && (
                <div className="space-y-4 mt-6 pt-6 border-t border-white/20">
                  <Select
                    value={selectedAiAction}
                    onValueChange={setSelectedAiAction}
                  >
                    <SelectTrigger className="w-full h-12 text-white text-base bg-white/10 border-white/20 focus:ring-white/30">
                      <SelectValue placeholder="Select AI action" />
                      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round" className="ml-2">
                        <path d="m6 9 6 6 6-6"/>
                      </svg>
                    </SelectTrigger>
                    <SelectContent className="bg-slate-800 border-white/20 text-white">
                      <SelectItem value="summarize" className="text-base focus:bg-white/10 focus:text-white">Summarize</SelectItem>
                      <SelectItem value="analyze" className="text-base focus:bg-white/10 focus:text-white">Analyze</SelectItem>
                    </SelectContent>
                  </Select>

                  <Button
                    variant="default"
                    className="w-full bg-gradient-to-r from-indigo-500 to-purple-600 hover:from-indigo-600 hover:to-purple-700 text-white h-12 text-base"
                    disabled={aiProcessing}
                    onClick={processWithAI}
                  >
                    {aiProcessing ? "Processing..." : `${selectedAiAction.charAt(0).toUpperCase() + selectedAiAction.slice(1)} Audio`}
                  </Button>
                  
                  {/* Display transcript in the recorder card */}
                  <div className="mt-4 p-4 bg-white/10 rounded-lg border border-white/20 text-sm text-white/90">
                    <h4 className="font-medium mb-2 text-white">Transcript</h4>
                    <div className="whitespace-pre-line max-h-60 overflow-y-auto">
                      {aiProcessing ? (
                        <div className="flex items-center justify-center py-4">
                          <div className="animate-pulse">Generating transcript...</div>
                        </div>
                      ) : (
                        getTranscriptContent(lastTranscriptionNumericId || 0)
                      )}
                    </div>
                  </div>
                </div>
              )}
            </TabsContent>

            <TabsContent value="upload" className="space-y-5">
              {!uploadedFile ? (
                <div className="flex flex-col items-center justify-center p-6 border-2 border-dashed border-white/20 rounded-lg bg-white/5">
                  <Upload className="h-8 w-8 text-white/50 mb-2" />
                  <p className="text-sm text-white/70 mb-4 text-center">
                    Upload an audio or video file to process with AI
                  </p>
                  <input
                    type="file"
                    id="audio-upload"
                    accept="audio/*,video/mp4"
                    className="hidden"
                    onChange={handleFileUpload}
                  />
                  <label
                    htmlFor="audio-upload"
                    className="inline-flex items-center justify-center px-4 py-2 bg-white/20 text-white text-sm font-medium rounded-md hover:bg-white/30 cursor-pointer transition-colors"
                  >
                    Select File
                  </label>
                </div>
              ) : (
                <div className="p-4 bg-white/10 rounded-lg border border-white/20">
                  <div className="flex items-center justify-between mb-4">
                    <div>
                      <h3 className="font-medium text-white">{uploadedFile.name}</h3>
                      <p className="text-xs text-white/60">
                        {(uploadedFile.size / 1024 / 1024).toFixed(2)} MB
                      </p>
                    </div>
                    <Button
                      variant="outline"
                      size="icon"
                      className="h-8 w-8 border-white/20 bg-white/10 text-white hover:bg-white/20 hover:text-white"
                      onClick={togglePlayUploaded}
                    >
                      {isUploadedPlaying ? <Pause className="h-4 w-4" /> : <Play className="h-4 w-4" />}
                    </Button>
                  </div>
                  
                  {/* Display transcript for uploaded file */}
                  <div className="mt-4 p-4 bg-white/10 rounded-lg border border-white/20 text-sm text-white/90">
                    <h4 className="font-medium mb-2 text-white">Transcript</h4>
                    <div className="whitespace-pre-line max-h-60 overflow-y-auto">
                      {aiProcessing ? (
                        <div className="flex items-center justify-center py-4">
                          <div className="animate-pulse">Generating transcript...</div>
                        </div>
                      ) : (
                        getTranscriptContent(lastTranscriptionNumericId || 0)
                      )}
                    </div>
                  </div>
                  
                  {/* AI Processing options for uploaded file */}
                  <div className="space-y-4 mt-6 pt-6 border-t border-white/20">
                    <Select
                      value={selectedAiAction}
                      onValueChange={setSelectedAiAction}
                    >
                      <SelectTrigger className="w-full h-12 text-white text-base bg-white/10 border-white/20 focus:ring-white/30">
                        <SelectValue placeholder="Select AI action" />
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round" className="ml-2">
                          <path d="m6 9 6 6 6-6"/>
                        </svg>
                      </SelectTrigger>
                      <SelectContent className="bg-slate-800 border-white/20 text-white">
                        <SelectItem value="summarize" className="text-base focus:bg-white/10 focus:text-white">Summarize</SelectItem>
                        <SelectItem value="analyze" className="text-base focus:bg-white/10 focus:text-white">Analyze</SelectItem>
                      </SelectContent>
                    </Select>

                    <Button
                      variant="default"
                      className="w-full bg-gradient-to-r from-indigo-500 to-purple-600 hover:from-indigo-600 hover:to-purple-700 text-white h-12 text-base"
                      disabled={aiProcessing}
                      onClick={processWithAI}
                    >
                      {aiProcessing ? "Processing..." : `${selectedAiAction.charAt(0).toUpperCase() + selectedAiAction.slice(1)} Audio`}
                    </Button>
                  </div>
                </div>
              )}
            </TabsContent>
          </Tabs>
        </CardContent>
        )}
        {/* Only show footer with save button after recording and when not minimized */}
        {isPostRecording && audioURL && !isMinimized && (
          <CardFooter className="px-5 py-3 border-t border-white/20 bg-white/5 text-xs text-white/60">
            <div className="w-full flex justify-between items-center">
              <span>Audio Recorder</span>
              <a
                href="#"
                className="text-white/80 hover:text-white hover:underline transition-colors"
                onClick={(e) => {
                  e.preventDefault()
                  if (audioURL) {
                    const a = document.createElement("a")
                    a.href = audioURL
                    const extension = getFileExtensionFromMimeType(currentMimeType);
                    a.download = `recording.${extension}`
                    a.click()
                  }
                }}
              >
                <Save className="h-4 w-4 inline-block mr-1" />
                Save Recording
              </a>
            </div>
          </CardFooter>
        )}
        {/* Audio element for playback */}
        <audio 
          ref={audioRef} 
          className="hidden" 
          controls={false}
          preload="auto"
          onError={(e) => console.error("Audio error:", e)}
        />
      </Card>

      {/* Modal for expanded card */}
      <Dialog open={modalOpen} onOpenChange={setModalOpen}>
        <DialogContent className="bg-white/10 backdrop-blur-md border border-white/20 text-white max-w-4xl min-h-[450px] max-h-[80vh] flex flex-col p-0 rounded-lg overflow-hidden shadow-lg">
          <div className="p-3 border-b border-white/20 bg-white/5 flex items-center">
            <h4 className="font-medium text-white text-base truncate flex-1">
              {selectedCard?.title || (selectedCard?.type === "summarize" ? "Summary" : "Analysis")}
            </h4>
            <DialogClose className="h-6 w-6 rounded-full bg-white/10 hover:bg-white/20 text-white/70 hover:text-white flex-shrink-0 ml-2 flex items-center justify-center">
              <X className="h-3 w-3" />
            </DialogClose>
          </div>
          <div className="p-3 flex-1 overflow-y-auto text-sm text-white/90 flex flex-col">
            {selectedCard ? (
              <>
                <div className="whitespace-pre-line flex-1">
                  {selectedCard.content.includes("This is a") ? 
                    selectedCard.content.replace(/^This is a (summary|analysis) of the audio recording\.\s+/, "") : 
                    selectedCard.content}
                </div>
                
                <div className="mt-4">
                  <div className="flex items-center p-2 hover:bg-white/5 rounded-lg cursor-pointer"
                    onClick={(e) => {
                      e.stopPropagation();
                      const transcriptEl = document.getElementById(`modal-transcript-${selectedCard.id}`);
                      if (transcriptEl) {
                        transcriptEl.classList.toggle('hidden');
                        
                        // If we're showing the transcript, ensure we have the content
                        if (!transcriptEl.classList.contains('hidden')) {
                          console.log(`AudioRecorder - Showing transcript for ID ${selectedCard.id}`);
                          
                          // If we don't have the content in the map, try to get it
                          if (!transcriptMap[selectedCard.id]) {
                            // Set a loading state while we fetch
                            setTranscriptMap(prevMap => ({
                              ...prevMap,
                              [selectedCard.id]: "Loading transcript from database..."
                            }));
                            
                            if (selectedCard.type === 'transcribe' && selectedCard.originalId) {
                              console.log(`AudioRecorder - Fetching transcript from DB for transcription ID ${selectedCard.originalId}`);
                              // Fetch from database async
                              fetchTranscriptionFromDB(selectedCard.originalId, selectedCard.id)
                                .then(content => {
                                  if (content) {
                                    setTranscriptMap(prevMap => ({
                                      ...prevMap,
                                      [selectedCard.id]: content
                                    }));
                                  }
                                });
                            } 
                            else if ((selectedCard.type === 'summarize' || selectedCard.type === 'analyze') && selectedCard.originalId) {
                              console.log(`AudioRecorder - Fetching transcript for analysis ID ${selectedCard.originalId}`);
                              // For summaries and analyses, we need to get the transcription via the analysis record
                              fetchTranscriptionForAnalysis(selectedCard.originalId, selectedCard.id)
                                .then(content => {
                                  if (content) {
                                    setTranscriptMap(prevMap => ({
                                      ...prevMap,
                                      [selectedCard.id]: content
                                    }));
                                  }
                                });
                            }
                            else if (selectedCard.content) {
                              // Fallback to using the content from the card
                              console.log(`AudioRecorder - Using content from card for ID ${selectedCard.id}`);
                              setTranscriptMap(prevMap => ({
                                ...prevMap,
                                [selectedCard.id]: selectedCard.content
                              }));
                            } else {
                              console.log(`AudioRecorder - No content available for ID ${selectedCard.id}`);
                              setTranscriptMap(prevMap => ({
                                ...prevMap,
                                [selectedCard.id]: "No transcript available"
                              }));
                            }
                          }
                        }
                      }
                    }}
                  >
                    <Button
                      variant="ghost"
                      size="icon"
                      className="h-5 w-5 rounded-full bg-white/10 hover:bg-red-500/20 text-white/70 hover:text-red-400 flex-shrink-0 mr-2"
                      onClick={(e) => {
                        e.stopPropagation();
                        // Find the original transcription ID
                        const transcriptionId = lastTranscriptionId;
                        if (transcriptionId) {
                          handleDeleteClick(selectedCard.id, 'transcribe', transcriptionId);
                        } else {
                          toast.error('Cannot delete transcript', { description: 'Original ID not found' });
                        }
                      }}
                    >
                      <Trash2 className="h-3 w-3" />
                    </Button>
                    <h5 className="font-medium text-white/90 text-sm flex-1 truncate">Original Transcript</h5>
                    <ChevronDown className="h-4 w-4 text-white/70 flex-shrink-0 ml-2" />
                  </div>
                  <div id={`modal-transcript-${selectedCard.id}`} className="p-3 bg-white/5 rounded-lg hidden">
                    <div className="whitespace-pre-line text-white/80 text-xs">
                      {transcriptMap[selectedCard.id] || "Loading transcript..."}
                    </div>
                  </div>
                </div>
              </>
            ) : (
              <div className="flex items-center justify-center h-full">
                <div className="animate-pulse">Loading...</div>
              </div>
            )}
          </div>
          
          {selectedCard && (
            <div className="px-4 py-3 border-t border-white/10 bg-white/5 text-xs text-white/60 flex justify-between">
              <span>{new Date(selectedCard.date || Date.now()).toLocaleDateString('en-US', { 
                year: 'numeric',
                month: 'short', 
                day: 'numeric',
                hour: '2-digit',
                minute: '2-digit'
              })}</span>
              <span>{selectedCard.type === "summarize" ? "Summary" : "Analysis"}</span>
            </div>
          )}
        </DialogContent>
      </Dialog>
    </>
  )
}

