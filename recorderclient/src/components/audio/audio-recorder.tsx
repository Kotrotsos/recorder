"use client"

import type React from "react"

import { useState, useRef, useEffect } from "react"
import { Upload, Mic, Square, Play, Pause, Save, Trash2, PlusCircle, X, Maximize2, Minimize2, ChevronDown, ChevronUp } from "lucide-react"
import { Button } from "@/components/ui/button"
import { Card, CardContent, CardFooter } from "@/components/ui/card"
import { Tabs, TabsContent, TabsList, TabsTrigger } from "@/components/ui/tabs"
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from "@/components/ui/select"
import { transcribeAudio, summarizeText, analyzeText } from "@/lib/api-client"
import { toast } from "sonner"

interface AudioRecorderProps {
  isAuthenticated: boolean;
  onResultsChange?: (results: Array<{ id: number; type: string; content: string; title?: string; generating: boolean; date?: string }>) => void;
}

export default function AudioRecorder({ isAuthenticated = false, onResultsChange }: AudioRecorderProps) {
  // Basic state
  const [isRecording, setIsRecording] = useState(false)
  const [audioURL, setAudioURL] = useState<string | null>(null)
  const [recordingTime, setRecordingTime] = useState(0)
  const [isPlaying, setIsPlaying] = useState(false)
  const [uploadedFile, setUploadedFile] = useState<File | null>(null)
  const [uploadedAudioURL, setUploadedAudioURL] = useState<string | null>(null)
  const [isUploadedPlaying, setIsUploadedPlaying] = useState(false)
  const [isPostRecording, setIsPostRecording] = useState(false)
  const [selectedAiAction, setSelectedAiAction] = useState<string>("summarize")
  const [aiProcessing, setAiProcessing] = useState(false)
  const [currentMimeType, setCurrentMimeType] = useState<string>("")
  const [processedResults, setProcessedResults] = useState<Array<{ id: number; type: string; content: string; title?: string; generating: boolean; expanded?: boolean; date?: string }>>([]);
  const [isMinimized, setIsMinimized] = useState<boolean>(false);
  
  // Recording time limit in seconds based on auth status
  const recordingTimeLimit = isAuthenticated ? 600 : 300; // 10 mins if logged in, 5 mins if not
  
  // Add a state for the transcript
  const [transcriptContent, setTranscriptContent] = useState<string>(
    "This is a simulated transcript of your audio recording. It would contain all the spoken words detected in your recording. In a real implementation, this would be generated by a speech-to-text service.\n\nThe transcript would be formatted with paragraphs and punctuation to make it easy to read. It might also include timestamps or speaker identification depending on the service used.",
  )

  // Refs
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const audioRef = useRef<HTMLAudioElement | null>(null)
  const timerRef = useRef<NodeJS.Timeout | null>(null)
  const micStreamRef = useRef<MediaStream | null>(null)
  
  // Visualization state for the recording button
  const [audioLevel, setAudioLevel] = useState<number>(0)
  const visualizationIntervalRef = useRef<NodeJS.Timeout | null>(null)
  
  // Effect to check if recording time has reached the limit
  useEffect(() => {
    if (isRecording && recordingTime >= recordingTimeLimit) {
      stopRecording();
      
      // Show toast notification
      if (isAuthenticated) {
        toast.warning("Recording limit reached", {
          description: "You've reached the 10-minute recording limit."
        });
      } else {
        toast.warning("Recording limit reached", {
          description: "You've reached the 5-minute recording limit. Sign in for longer recordings."
        });
      }
    }
  }, [isRecording, recordingTime, recordingTimeLimit, isAuthenticated]);
  
  // Calculate time remaining in seconds
  const timeRemainingSeconds = recordingTimeLimit - recordingTime;
  
  // Format time remaining for display
  const formatTimeRemaining = () => {
    const mins = Math.floor(timeRemainingSeconds / 60);
    const secs = timeRemainingSeconds % 60;
    return `${mins}:${secs < 10 ? "0" : ""}${secs}`;
  };
  
  // Clean up resources when component unmounts
  useEffect(() => {
    return () => {
      // Stop any active recording
      if (mediaRecorderRef.current && mediaRecorderRef.current.state === "recording") {
        try {
          mediaRecorderRef.current.stop()
        } catch (e) {
          console.error("Error stopping media recorder:", e)
        }
      }
      
      // Clear timer
      if (timerRef.current) {
        clearInterval(timerRef.current)
      }
      
      // Clear visualization interval
      if (visualizationIntervalRef.current) {
        clearInterval(visualizationIntervalRef.current)
      }
      
      // Stop microphone stream
      if (micStreamRef.current) {
        try {
          micStreamRef.current.getTracks().forEach(track => {
            try {
              track.stop()
            } catch (e) {
              console.error("Error stopping track:", e)
            }
          })
        } catch (e) {
          console.error("Error stopping stream:", e)
        }
      }
      
      // Clean up audio URLs
      if (audioURL) {
        URL.revokeObjectURL(audioURL)
      }
      
      if (uploadedAudioURL) {
        URL.revokeObjectURL(uploadedAudioURL)
      }
    }
  }, [audioURL, uploadedAudioURL])
  
  // Handle audio element events
  useEffect(() => {
    const audioElement = audioRef.current
    
    if (audioElement) {
      const handleEnded = () => {
        console.log("Audio playback ended")
        setIsPlaying(false)
        setIsUploadedPlaying(false)
      }
      
      const handleError = (e: Event) => {
        console.error("Audio element error:", e)
        setIsPlaying(false)
        setIsUploadedPlaying(false)
      }
      
      audioElement.addEventListener('ended', handleEnded)
      audioElement.addEventListener('error', handleError)
      
      return () => {
        audioElement.removeEventListener('ended', handleEnded)
        audioElement.removeEventListener('error', handleError)
      }
    }
  }, [])
  
  // Format time for display
  const formatTime = (seconds: number) => {
    const mins = Math.floor(seconds / 60)
    const secs = seconds % 60
    return `${mins}:${secs < 10 ? "0" : ""}${secs}`
  }
  
  // Start recording
  const startRecording = async () => {
    try {
      console.log("Starting recording...")
      
      // Get microphone stream
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        }
      })
      
      // Store stream reference
      micStreamRef.current = stream
      
      // Reset chunks array
      audioChunksRef.current = []
      
      // Determine the supported MIME type
      const mimeType = getSupportedMimeType();
      console.log("Using MIME type:", mimeType);
      setCurrentMimeType(mimeType || 'audio/mp4');
      
      // Create media recorder with the supported MIME type
      try {
        const options = mimeType ? { mimeType } : { mimeType: 'audio/mp4' };
        mediaRecorderRef.current = new MediaRecorder(stream, options);
        console.log("MediaRecorder created successfully");
      } catch (error) {
        console.error("Error creating MediaRecorder:", error);
        // Try again without specifying a MIME type
        try {
          console.log("Trying to create MediaRecorder without MIME type");
          mediaRecorderRef.current = new MediaRecorder(stream);
          console.log("MediaRecorder created successfully without MIME type");
          // Get the actual MIME type being used
          if (mediaRecorderRef.current.mimeType) {
            setCurrentMimeType(mediaRecorderRef.current.mimeType);
            console.log("Using browser-selected MIME type:", mediaRecorderRef.current.mimeType);
          } else {
            setCurrentMimeType('audio/mp4'); // Default fallback
          }
        } catch (fallbackError) {
          console.error("Failed to create MediaRecorder even without MIME type:", fallbackError);
          alert("Your browser doesn't support audio recording. Please try a different browser.");
          // Clean up
          if (micStreamRef.current) {
            micStreamRef.current.getTracks().forEach(track => track.stop());
          }
          return;
        }
      }
      
      // Set up event handlers
      mediaRecorderRef.current.ondataavailable = (event) => {
        console.log("Data available:", event.data?.size)
        if (event.data && event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }
      
      mediaRecorderRef.current.onstop = () => {
        console.log("MediaRecorder stopped, chunks:", audioChunksRef.current.length);
        if (audioChunksRef.current.length === 0) {
          console.warn("No audio data was recorded");
          return;
        }
        try {
          console.log("Creating audio blob with MIME type: audio/mp4");
          const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/mp4' });
          const url = URL.createObjectURL(audioBlob);
          setAudioURL(url);
          console.log("Audio blob created successfully");

          // Start automatic transcription
          setAiProcessing(true);
          (async () => {
            try {
              console.log("Calling transcribeAudio with audio blob");
              const result = await transcribeAudio(audioBlob);
              let transcript = "";
              if (result.success && result.text) {
                transcript = result.text;
              } else {
                transcript = "Error: " + (result.error || "Unknown transcription error");
              }
              setTranscriptContent(transcript);
            } catch (e) {
              console.error("Error in transcription process:", e);
              setTranscriptContent("Error: " + (e instanceof Error ? e.message : String(e)));
            } finally {
              setAiProcessing(false);
            }
          })();

        } catch (error) {
          console.error("Error creating audio blob:", error);
          // Fallback logic
          try {
            const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/mp4' });
            const url = URL.createObjectURL(audioBlob);
            setAudioURL(url);
            setCurrentMimeType('audio/mp4');
            console.log("Created audio blob with fallback MIME type: audio/mp4");
          } catch (fallbackError) {
            console.error("Failed to create audio blob even with fallback:", fallbackError);
            alert("There was an error processing your recording.");
          }
        }
      }
      
      // Start recording
      mediaRecorderRef.current.start(100) // Capture in 100ms chunks
      
      // Update UI state
      setIsRecording(true)
      setRecordingTime(0)
      setIsPostRecording(false)
      
      // Start timer
      timerRef.current = setInterval(() => {
        setRecordingTime(prev => prev + 1)
      }, 1000)
      
      // Start visualization for the recording button
      startVisualization(stream)
      
      console.log("Recording started successfully")
    } catch (error) {
      console.error("Error starting recording:", error)
    }
  }
  
  // Start visualization for the recording button
  const startVisualization = (stream: MediaStream) => {
    try {
      // Clear any existing visualization interval
      if (visualizationIntervalRef.current) {
        clearInterval(visualizationIntervalRef.current)
      }
      
      // Create audio context and analyzer
      // Define the AudioContext type that includes webkitAudioContext
      type AudioContextType = typeof AudioContext
      
      // Define a type for the window with webkitAudioContext
      interface WindowWithWebkitAudio extends Window {
        webkitAudioContext?: AudioContextType;
      }
      
      const AudioContextClass: AudioContextType = 
        window.AudioContext || (window as WindowWithWebkitAudio).webkitAudioContext || null as unknown as AudioContextType;
      
      const audioContext = new AudioContextClass()
      const analyser = audioContext.createAnalyser()
      analyser.fftSize = 256
      analyser.smoothingTimeConstant = 0.3 // Make it more responsive (lower = more responsive)
      
      // Connect microphone to analyzer
      const source = audioContext.createMediaStreamSource(stream)
      source.connect(analyser)
      
      // Create data array for frequency data
      const dataArray = new Uint8Array(analyser.frequencyBinCount)
      
      // Update visualization at regular intervals
      visualizationIntervalRef.current = setInterval(() => {
        // Get frequency data
        analyser.getByteFrequencyData(dataArray)
        
        // Calculate average level with emphasis on lower frequencies
        // which are more common in speech
        let sum = 0
        let weight = 0
        for (let i = 0; i < dataArray.length; i++) {
          // Give more weight to lower frequencies (first third of the spectrum)
          const frequencyWeight = i < dataArray.length / 3 ? 3 : 1
          sum += dataArray[i] * frequencyWeight
          weight += frequencyWeight
        }
        const avg = sum / weight
        
        // Update audio level (0-100 scale) with smoother transitions
        // Amplify the effect by multiplying the raw value
        setAudioLevel(prev => {
          // Smooth transitions by blending previous and new values
          const amplifiedLevel = Math.min(100, avg * 1.5) // Amplify by 1.5x
          return prev * 0.2 + amplifiedLevel * 0.8 // 80% new value, 20% old value for more responsiveness
        })
      }, 20) // Update 50 times per second for smoother animation
      
      // Clean up when recording stops
      return () => {
        clearInterval(visualizationIntervalRef.current!)
        audioContext.close()
      }
    } catch (error) {
      console.error("Error starting visualization:", error)
    }
  }
  
  // Get dynamic gradient position based on audio level
  const getGradientPosition = () => {
    // Map audio level (0-100) to gradient position (100%-0%)
    // When audio level is high, gradient moves up (lower percentage)
    // When audio level is low, gradient moves down (higher percentage)
    const position = 100 - audioLevel;
    return `${position}%`
  }
  
  // Get button styles for recording button
  const getRecordingButtonStyles = () => {
    // Fixed size for the button
    const size = '4rem';
    
    // Create a dynamic background with gradient that moves based on audio level
    return {
      width: size,
      height: size,
      background: `linear-gradient(to top, #ef4444 ${getGradientPosition()}, #f87171 100%)`,
      boxShadow: `0 0 ${Math.max(5, audioLevel / 5)}px rgba(239, 68, 68, 0.6)`,
      transition: 'box-shadow 0.1s ease-in-out',
    };
  }
  
  // Get button styles for stop button
  const getStopButtonStyles = () => {
    // Fixed size for the button
    const size = '4rem';
    
    // Create a dynamic background with gradient that moves based on audio level
    return {
      width: size,
      height: size,
      background: `linear-gradient(to top, #b91c1c ${getGradientPosition()}, #ef4444 100%)`,
      boxShadow: `0 0 ${Math.max(5, audioLevel / 5)}px rgba(185, 28, 28, 0.6)`,
      transition: 'box-shadow 0.1s ease-in-out',
    };
  }
  
  // Stop recording
  const stopRecording = () => {
    try {
      console.log("Stopping recording...")
      
      // Stop timer
      if (timerRef.current) {
        clearInterval(timerRef.current)
        timerRef.current = null
      }
      
      // Stop visualization
      if (visualizationIntervalRef.current) {
        clearInterval(visualizationIntervalRef.current)
        visualizationIntervalRef.current = null
      }
      
      // Stop media recorder
      if (mediaRecorderRef.current && mediaRecorderRef.current.state === "recording") {
        mediaRecorderRef.current.stop()
      }
      
      // Stop microphone stream
      if (micStreamRef.current) {
        micStreamRef.current.getTracks().forEach(track => track.stop())
        micStreamRef.current = null
      }
      
      // Update UI state
      setIsRecording(false)
      setIsPostRecording(true)
      setAudioLevel(0)
      
      console.log("Recording stopped successfully")
    } catch (error) {
      console.error("Error stopping recording:", error)
      setIsRecording(false)
      setIsPostRecording(true)
    }
  }
  
  // Reset recorder to initial state
  const resetRecorder = () => {
    // Clear previous recording
    if (audioURL) {
      URL.revokeObjectURL(audioURL)
      setAudioURL(null)
    }
    
    // Reset to initial state
    setIsPostRecording(false)
    setIsRecording(false)
    setRecordingTime(0)
    setAiProcessing(false)
    setIsPlaying(false)
    
    // Stop audio playback if it's playing
    if (audioRef.current) {
      audioRef.current.pause();
      audioRef.current.currentTime = 0;
    }
  }
  
  // Handle file upload
  const handleFileUpload = (event: React.ChangeEvent<HTMLInputElement>) => {
    const files = event.target.files
    if (files && files.length > 0) {
      const file = files[0]
      
      // Check if the file is an audio or video file (MP4 files are often identified as video/mp4)
      if (!file.type.startsWith('audio/') && !file.type.startsWith('video/')) {
        alert('Please upload an audio or video file')
        return
      }
      
      console.log("Uploaded file type:", file.type)
      setUploadedFile(file)
      
      // Create a URL for the uploaded file
      const url = URL.createObjectURL(file)
      
      // If we previously had an uploaded file, revoke its URL
      if (uploadedAudioURL) {
        URL.revokeObjectURL(uploadedAudioURL)
      }
      
      setUploadedAudioURL(url)
      setCurrentMimeType('audio/mp4') // Treat all uploads as MP4 for consistency
      
      // Reset any previous recording state
      if (isPostRecording) {
        resetRecorder()
      }
      
      // Start automatic transcription of the uploaded file
      setAiProcessing(true);
      
      // Convert File to Blob for transcription
      (async () => {
        try {
          console.log("Calling transcribeAudio with uploaded file");
          const result = await transcribeAudio(file);
          let transcript = "";
          if (result.success && result.text) {
            transcript = result.text;
            toast.success("Transcription complete", {
              description: "Your uploaded file has been transcribed successfully."
            });
          } else {
            transcript = "Error: " + (result.error || "Unknown transcription error");
            toast.error("Transcription failed", {
              description: result.error || "Unknown transcription error"
            });
          }
          setTranscriptContent(transcript);
        } catch (e) {
          console.error("Error in transcription process:", e);
          setTranscriptContent("Error: " + (e instanceof Error ? e.message : String(e)));
          toast.error("Transcription failed", {
            description: e instanceof Error ? e.message : String(e)
          });
        } finally {
          setAiProcessing(false);
        }
      })();
    }
  }
  
  // Toggle play recorded audio
  const togglePlayRecorded = () => {
    if (audioRef.current && audioURL) {
      if (isPlaying) {
        audioRef.current.pause()
      } else {
        // Make sure we reset the audio element before setting a new source
        audioRef.current.pause()
        audioRef.current.currentTime = 0
        audioRef.current.src = audioURL
        
        // Add error handling for playback
        const playPromise = audioRef.current.play()
        if (playPromise !== undefined) {
          playPromise.catch(error => {
            console.error("Error playing recorded audio:", error)
            alert("Error playing audio. Please try recording again.")
            setIsPlaying(false)
          })
        }
      }
      setIsPlaying(!isPlaying)
    }
  }
  
  // Toggle play uploaded audio
  const togglePlayUploaded = () => {
    if (audioRef.current && uploadedAudioURL) {
      if (isUploadedPlaying) {
        audioRef.current.pause()
      } else {
        // Make sure we reset the audio element before setting a new source
        audioRef.current.pause()
        audioRef.current.currentTime = 0
        audioRef.current.src = uploadedAudioURL
        
        // Add error handling for playback
        const playPromise = audioRef.current.play()
        if (playPromise !== undefined) {
          playPromise.catch(error => {
            console.error("Error playing uploaded audio:", error)
            alert("Error playing audio. The file format may not be supported.")
            setIsUploadedPlaying(false)
          })
        }
      }
      setIsUploadedPlaying(!isUploadedPlaying)
    }
  }
  
  // Process with AI
  const processWithAI = async () => {
    setAiProcessing(true);
    const newId = new Date().getTime();
    
    // Only add to processedResults for summarize and analyze actions, not for transcripts
    if (selectedAiAction !== "transcribe") {
      // Add a new result card with generating state
      setProcessedResults(prev => [
        ...prev,
        { 
          id: newId, 
          type: selectedAiAction, 
          content: "", 
          title: "", 
          generating: true, 
          expanded: false,
          date: new Date().toISOString() // Add date field
        }
      ]);
    }

    try {
      if (selectedAiAction === "summarize") {
        const result = await summarizeText(transcriptContent);
        
        if (result.success) {
          // Update the result card with the generated content
          setProcessedResults(prev => prev.map(item => 
            item.id === newId 
              ? { 
                  ...item, 
                  content: result.content || result.result || "", 
                  title: result.title || "Summary", 
                  generating: false 
                } 
              : item
          ));
        } else {
          // Handle error
          setProcessedResults(prev => prev.map(item => 
            item.id === newId 
              ? { 
                  ...item, 
                  content: `Error: ${result.error || "Failed to generate summary"}`, 
                  title: "Error", 
                  generating: false 
                } 
              : item
          ));
        }
      } else if (selectedAiAction === "analyze") {
        const result = await analyzeText(transcriptContent);
        
        if (result.success) {
          // Update the result card with the generated content
          setProcessedResults(prev => prev.map(item => 
            item.id === newId 
              ? { 
                  ...item, 
                  content: result.content || result.result || "", 
                  title: result.title || "Analysis", 
                  generating: false 
                } 
              : item
          ));
        } else {
          // Handle error
          setProcessedResults(prev => prev.map(item => 
            item.id === newId 
              ? { 
                  ...item, 
                  content: `Error: ${result.error || "Failed to generate analysis"}`, 
                  title: "Error", 
                  generating: false 
                } 
              : item
          ));
        }
      }
    } catch (error) {
      console.error("Error in AI processing:", error);
      
      // Update the result card with the error
      setProcessedResults(prev => prev.map(item => 
        item.id === newId 
          ? { 
              ...item, 
              content: `Error: ${error instanceof Error ? error.message : String(error)}`, 
              title: "Error", 
              generating: false 
            } 
          : item
      ));
    } finally {
      setAiProcessing(false);
    }
  };

  // Helper function to determine the supported MIME type
  const getSupportedMimeType = () => {
    // Only support MP4 format as requested
    if (MediaRecorder.isTypeSupported('audio/mp4')) {
      console.log('Browser supports recording in audio/mp4 format');
      return 'audio/mp4';
    }
    
    console.warn('MP4 format not supported. Using default browser implementation.');
    // Fallback to default (browser will choose)
    return '';
  };

  // Helper function to get file extension from MIME type
  const getFileExtensionFromMimeType = (mimeType: string): string => {
    // Default to mp4 as requested
    if (!mimeType || mimeType.includes('mp4')) return 'mp4';
    if (mimeType.includes('webm')) return 'webm';
    if (mimeType.includes('ogg')) return 'ogg';
    if (mimeType.includes('wav')) return 'wav';
    if (mimeType.includes('mp3') || mimeType.includes('mpeg')) return 'mp3';
    if (mimeType.includes('aac')) return 'aac';
    
    // Default fallback
    return 'mp4';
  };

  // Call onResultsChange when processedResults changes
  useEffect(() => {
    if (onResultsChange) {
      onResultsChange(processedResults);
    }
  }, [processedResults, onResultsChange]);

  // Function to toggle expanded state for a card
  const toggleCardExpanded = (id: number) => {
    setProcessedResults(prev => 
      prev.map(result => 
        result.id === id ? { ...result, expanded: !result.expanded } : result
      )
    );
  };

  return (
    <>
      {processedResults.length > 0 && (
        <div className="fixed top-32 left-0 right-0 px-8 z-[60] overflow-y-auto" style={{ maxHeight: 'calc(100vh - 200px)' }}>
          <div className="grid grid-cols-1 sm:grid-cols-1 md:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4 gap-6 pb-32">
            {processedResults.filter(result => result.type !== "transcribe").map(result => (
              <div key={result.id} className={`${result.expanded ? 'col-span-full' : ''} transition-all duration-300`}>
                <div 
                  className={`w-full ${result.expanded ? 'min-h-[400px]' : 'h-[230px]'} bg-white/10 backdrop-blur-md rounded-lg border border-white/20 shadow-lg overflow-hidden flex flex-col ${result.generating ? "opacity-50" : "opacity-100"}`}
                >
                  <div className="p-3 border-b border-white/20 bg-white/5 flex justify-between items-center">
                    <h4 className="font-medium text-white text-base truncate max-w-[85%]">
                      {result.generating ? 
                        (result.type === "summarize" ? "Generating Summary..." : "Generating Analysis...") : 
                        (result.title || (result.type === "summarize" ? "Summary" : "Analysis"))}
                    </h4>
                    <Button
                      variant="ghost"
                      size="icon"
                      className="h-6 w-6 rounded-full bg-white/10 hover:bg-white/20 text-white/70 hover:text-white"
                      onClick={() => toggleCardExpanded(result.id)}
                      disabled={result.generating}
                    >
                      {result.expanded ? <Minimize2 className="h-3 w-3" /> : <Maximize2 className="h-3 w-3" />}
                    </Button>
                  </div>
                  <div className={`p-3 flex-1 overflow-y-auto text-sm text-white/90 ${result.expanded ? 'flex flex-col' : ''}`}>
                    {result.generating ? (
                      <div className="flex items-center justify-center h-full">
                        <div className="animate-pulse">Generating...</div>
                      </div>
                    ) : (
                      <>
                        <div className={`whitespace-pre-line ${result.expanded ? 'flex-1' : 'line-clamp-7'}`}>
                          {result.content.includes("This is a") ? 
                            result.content.replace(/^This is a (summary|analysis) of the audio recording\.\s+/, "") : 
                            result.content}
                        </div>
                        
                        {result.expanded && (
                          <div className="mt-4">
                            <div 
                              className="p-3 bg-white/5 rounded-lg mb-2 flex justify-between items-center cursor-pointer"
                              onClick={() => {
                                const transcriptEl = document.getElementById(`transcript-${result.id}`);
                                if (transcriptEl) {
                                  transcriptEl.classList.toggle('hidden');
                                }
                              }}
                            >
                              <h5 className="font-medium text-white/90 text-sm">Original Transcript</h5>
                              <ChevronDown className="h-4 w-4 text-white/70" />
                            </div>
                            <div id={`transcript-${result.id}`} className="p-3 bg-white/5 rounded-lg hidden">
                              <div className="whitespace-pre-line text-white/80 text-xs">
                                {transcriptContent}
                              </div>
                            </div>
                          </div>
                        )}
                        
                        {!result.expanded && (
                          <div className="text-xs text-white/50 absolute bottom-3 left-3 pt-5">
                            <span>{new Date(result.date || Date.now()).toLocaleDateString('en-US', { 
                              month: 'short', 
                              day: 'numeric',
                              hour: '2-digit',
                              minute: '2-digit'
                            })}</span>
                          </div>
                        )}
                      </>
                    )}
                  </div>
                  
                  {result.expanded && (
                    <div className="px-4 py-3 border-t border-white/10 bg-white/5 text-xs text-white/60 flex justify-between">
                      <span>{new Date(result.date || Date.now()).toLocaleDateString('en-US', { 
                        year: 'numeric',
                        month: 'short', 
                        day: 'numeric',
                        hour: '2-digit',
                        minute: '2-digit'
                      })}</span>
                      <span>{result.type === "summarize" ? "Summary" : "Analysis"}</span>
                    </div>
                  )}
                </div>
              </div>
            ))}
          </div>
        </div>
      )}
      <Card className={`fixed bottom-4 left-1/2 transform -translate-x-1/2 ${isMinimized ? 'w-auto' : 'w-full max-w-md'} overflow-hidden bg-white/10 backdrop-blur-md border-0 shadow-xl z-[100] transition-all duration-300`}>
        <div className="absolute top-2 right-2 z-10">
          <Button
            variant="ghost"
            size="icon"
            className="h-6 w-6 rounded-full bg-white/10 hover:bg-white/20 text-white/70 hover:text-white"
            onClick={() => setIsMinimized(!isMinimized)}
          >
            {isMinimized ? <Maximize2 className="h-3 w-3" /> : <X className="h-3 w-3" />}
          </Button>
        </div>
        {isMinimized ? (
          <div className="p-3 flex items-center space-x-3">
            <Button
              variant="default"
              style={getRecordingButtonStyles()}
              className="rounded-full p-0 flex items-center justify-center h-10 w-10"
              onClick={() => {
                if (isRecording) {
                  stopRecording();
                } else {
                  setIsMinimized(false);
                  startRecording();
                }
              }}
            >
              {isRecording ? <Square className="h-4 w-4" /> : <Mic className="h-4 w-4" />}
            </Button>
            <span className="text-xs text-white/80 font-medium">
              {isRecording ? formatTimeRemaining() : "Record Audio"}
            </span>
          </div>
        ) : (
        <CardContent className="p-5">
          <Tabs defaultValue="record" className="w-full">
            <TabsList className="grid w-full grid-cols-2 mb-6 bg-white/10 p-1 rounded-lg">
              <TabsTrigger
                value="record"
                className="data-[state=active]:bg-white/20 data-[state=active]:text-white data-[state=active]:shadow-md text-white/70"
              >
                Record
              </TabsTrigger>
              <TabsTrigger
                value="upload"
                className="data-[state=active]:bg-white/20 data-[state=active]:text-white data-[state=active]:shadow-md text-white/70"
              >
                Upload
              </TabsTrigger>
            </TabsList>

            <TabsContent value="record" className="space-y-5">
              {/* Recording controls */}
              <div className="space-y-4">
                {/* Timer display - show time remaining when recording, or elapsed time after recording */}
                {isRecording ? (
                  <div className="text-center">
                    <div className="text-3xl font-mono font-bold text-white">
                      {formatTimeRemaining()}
                    </div>
                    <div className="text-xs text-white/70 mt-1">
                      {isAuthenticated 
                        ? "Premium: 10 minute recording limit" 
                        : "Free: 5 minute recording limit"}
                    </div>
                  </div>
                ) : isPostRecording && (
                  <div className="text-center">
                    <span className="text-3xl font-mono font-bold text-white">
                      {formatTime(recordingTime)}
                    </span>
                  </div>
                )}

                {/* Control buttons */}
                <div className="flex justify-center gap-4">
                  {isRecording ? (
                    <Button
                      variant="destructive"
                      style={getStopButtonStyles()}
                      className="rounded-full p-0 flex items-center justify-center"
                      onClick={stopRecording}
                    >
                      <Square className="h-6 w-6" />
                    </Button>
                  ) : isPostRecording ? (
                    <div className="flex gap-3">
                      <Button
                        variant="outline"
                        size="icon"
                        className="rounded-full w-12 h-12 border-white/20 bg-white/10 text-white hover:bg-white/20 hover:text-white"
                        onClick={resetRecorder}
                      >
                        <Trash2 className="h-5 w-5" />
                      </Button>
                      <Button
                        variant="outline"
                        size="icon"
                        className="rounded-full w-12 h-12 border-white/20 bg-white/10 text-white hover:bg-white/20 hover:text-white"
                        onClick={togglePlayRecorded}
                      >
                        {isPlaying ? <Pause className="h-5 w-5" /> : <Play className="h-5 w-5" />}
                      </Button>
                      <Button
                        variant="outline"
                        size="icon"
                        className="rounded-full w-12 h-12 border-white/20 bg-white/10 text-white hover:bg-white/20 hover:text-white"
                        onClick={resetRecorder}
                      >
                        <PlusCircle className="h-5 w-5" />
                      </Button>
                    </div>
                  ) : (
                    <div className="flex flex-col items-center">
                      <Button
                        variant="default"
                        style={getRecordingButtonStyles()}
                        className="rounded-full p-0 flex items-center justify-center mb-2"
                        onClick={startRecording}
                      >
                        <Mic className="h-6 w-6" />
                      </Button>
                      <span className="text-xs text-white/60">Tap to record</span>
                    </div>
                  )}
                </div>
              </div>

              {/* AI Processing section - only show after recording */}
              {isPostRecording && audioURL && (
                <div className="space-y-4 mt-6 pt-6 border-t border-white/20">
                  <Select
                    value={selectedAiAction}
                    onValueChange={setSelectedAiAction}
                  >
                    <SelectTrigger className="w-full h-12 text-white text-base bg-white/10 border-white/20 focus:ring-white/30">
                      <SelectValue placeholder="Select AI action" />
                      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round" className="ml-2">
                        <path d="m6 9 6 6 6-6"/>
                      </svg>
                    </SelectTrigger>
                    <SelectContent className="bg-slate-800 border-white/20 text-white">
                      <SelectItem value="summarize" className="text-base focus:bg-white/10 focus:text-white">Summarize</SelectItem>
                      <SelectItem value="analyze" className="text-base focus:bg-white/10 focus:text-white">Analyze</SelectItem>
                    </SelectContent>
                  </Select>

                  <Button
                    variant="default"
                    className="w-full bg-gradient-to-r from-indigo-500 to-purple-600 hover:from-indigo-600 hover:to-purple-700 text-white h-12 text-base"
                    disabled={aiProcessing}
                    onClick={processWithAI}
                  >
                    {aiProcessing ? "Processing..." : `${selectedAiAction.charAt(0).toUpperCase() + selectedAiAction.slice(1)} Audio`}
                  </Button>
                  
                  {/* Display transcript in the recorder card */}
                  <div className="mt-4 p-4 bg-white/10 rounded-lg border border-white/20 text-sm text-white/90">
                    <h4 className="font-medium mb-2 text-white">Transcript</h4>
                    <div className="whitespace-pre-line max-h-60 overflow-y-auto">
                      {aiProcessing ? (
                        <div className="flex items-center justify-center py-4">
                          <div className="animate-pulse">Generating transcript...</div>
                        </div>
                      ) : (
                        transcriptContent
                      )}
                    </div>
                  </div>
                </div>
              )}
            </TabsContent>

            <TabsContent value="upload" className="space-y-5">
              {!uploadedFile ? (
                <div className="flex flex-col items-center justify-center p-6 border-2 border-dashed border-white/20 rounded-lg bg-white/5">
                  <Upload className="h-8 w-8 text-white/50 mb-2" />
                  <p className="text-sm text-white/70 mb-4 text-center">
                    Upload an audio or video file to process with AI
                  </p>
                  <input
                    type="file"
                    id="audio-upload"
                    accept="audio/*,video/mp4"
                    className="hidden"
                    onChange={handleFileUpload}
                  />
                  <label
                    htmlFor="audio-upload"
                    className="inline-flex items-center justify-center px-4 py-2 bg-white/20 text-white text-sm font-medium rounded-md hover:bg-white/30 cursor-pointer transition-colors"
                  >
                    Select File
                  </label>
                </div>
              ) : (
                <div className="p-4 bg-white/10 rounded-lg border border-white/20">
                  <div className="flex items-center justify-between mb-4">
                    <div>
                      <h3 className="font-medium text-white">{uploadedFile.name}</h3>
                      <p className="text-xs text-white/60">
                        {(uploadedFile.size / 1024 / 1024).toFixed(2)} MB
                      </p>
                    </div>
                    <Button
                      variant="outline"
                      size="icon"
                      className="h-8 w-8 border-white/20 bg-white/10 text-white hover:bg-white/20 hover:text-white"
                      onClick={togglePlayUploaded}
                    >
                      {isUploadedPlaying ? <Pause className="h-4 w-4" /> : <Play className="h-4 w-4" />}
                    </Button>
                  </div>
                  
                  {/* Display transcript for uploaded file */}
                  <div className="mt-4 p-4 bg-white/10 rounded-lg border border-white/20 text-sm text-white/90">
                    <h4 className="font-medium mb-2 text-white">Transcript</h4>
                    <div className="whitespace-pre-line max-h-60 overflow-y-auto">
                      {aiProcessing ? (
                        <div className="flex items-center justify-center py-4">
                          <div className="animate-pulse">Generating transcript...</div>
                        </div>
                      ) : (
                        transcriptContent
                      )}
                    </div>
                  </div>
                  
                  {/* AI Processing options for uploaded file */}
                  <div className="space-y-4 mt-6 pt-6 border-t border-white/20">
                    <Select
                      value={selectedAiAction}
                      onValueChange={setSelectedAiAction}
                    >
                      <SelectTrigger className="w-full h-12 text-white text-base bg-white/10 border-white/20 focus:ring-white/30">
                        <SelectValue placeholder="Select AI action" />
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round" className="ml-2">
                          <path d="m6 9 6 6 6-6"/>
                        </svg>
                      </SelectTrigger>
                      <SelectContent className="bg-slate-800 border-white/20 text-white">
                        <SelectItem value="summarize" className="text-base focus:bg-white/10 focus:text-white">Summarize</SelectItem>
                        <SelectItem value="analyze" className="text-base focus:bg-white/10 focus:text-white">Analyze</SelectItem>
                      </SelectContent>
                    </Select>

                    <Button
                      variant="default"
                      className="w-full bg-gradient-to-r from-indigo-500 to-purple-600 hover:from-indigo-600 hover:to-purple-700 text-white h-12 text-base"
                      disabled={aiProcessing}
                      onClick={processWithAI}
                    >
                      {aiProcessing ? "Processing..." : `${selectedAiAction.charAt(0).toUpperCase() + selectedAiAction.slice(1)} Audio`}
                    </Button>
                  </div>
                </div>
              )}
            </TabsContent>
          </Tabs>
        </CardContent>
        )}
        {/* Only show footer with save button after recording and when not minimized */}
        {isPostRecording && audioURL && !isMinimized && (
          <CardFooter className="px-5 py-3 border-t border-white/20 bg-white/5 text-xs text-white/60">
            <div className="w-full flex justify-between items-center">
              <span>Audio Recorder</span>
              <a
                href="#"
                className="text-white/80 hover:text-white hover:underline transition-colors"
                onClick={(e) => {
                  e.preventDefault()
                  if (audioURL) {
                    const a = document.createElement("a")
                    a.href = audioURL
                    const extension = getFileExtensionFromMimeType(currentMimeType);
                    a.download = `recording.${extension}`
                    a.click()
                  }
                }}
              >
                <Save className="h-4 w-4 inline-block mr-1" />
                Save Recording
              </a>
            </div>
          </CardFooter>
        )}
        {/* Audio element for playback */}
        <audio 
          ref={audioRef} 
          className="hidden" 
          controls={false}
          preload="auto"
          onError={(e) => console.error("Audio error:", e)}
        />
      </Card>
    </>
  )
}

