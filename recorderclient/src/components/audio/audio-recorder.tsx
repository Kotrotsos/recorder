"use client"

import type React from "react"

import { useState, useRef, useEffect } from "react"
import { Upload, Mic, Square, Play, Pause, Save, Trash2, PlusCircle, X, Maximize2, Minimize2, ChevronDown, ChevronUp } from "lucide-react"
import { Button } from "@/components/ui/button"
import { Card, CardContent, CardFooter } from "@/components/ui/card"
import { Tabs, TabsContent, TabsList, TabsTrigger } from "@/components/ui/tabs"
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from "@/components/ui/select"
import { transcribeAudio } from "@/lib/api-client"

interface AudioRecorderProps {
  isAuthenticated: boolean;
  onResultsChange?: (results: Array<{ id: number; type: string; content: string; title?: string; generating: boolean; date?: string }>) => void;
}

export default function AudioRecorder({ isAuthenticated = false, onResultsChange }: AudioRecorderProps) {
  // Basic state
  const [isRecording, setIsRecording] = useState(false)
  const [audioURL, setAudioURL] = useState<string | null>(null)
  const [recordingTime, setRecordingTime] = useState(0)
  const [isPlaying, setIsPlaying] = useState(false)
  const [uploadedFile, setUploadedFile] = useState<File | null>(null)
  const [uploadedAudioURL, setUploadedAudioURL] = useState<string | null>(null)
  const [isUploadedPlaying, setIsUploadedPlaying] = useState(false)
  const [isPostRecording, setIsPostRecording] = useState(false)
  const [selectedAiAction, setSelectedAiAction] = useState<string>("summarize")
  const [aiProcessing, setAiProcessing] = useState(false)
  const [currentMimeType, setCurrentMimeType] = useState<string>("")
  const [processedResults, setProcessedResults] = useState<Array<{ id: number; type: string; content: string; title?: string; generating: boolean; expanded?: boolean; date?: string }>>([]);
  const [isMinimized, setIsMinimized] = useState<boolean>(false);
  
  // Recording time limit in seconds based on auth status
  const recordingTimeLimit = isAuthenticated ? 600 : 300; // 10 mins if logged in, 5 mins if not
  
  // Add a state for the transcript
  const [transcriptContent, setTranscriptContent] = useState<string>(
    "This is a simulated transcript of your audio recording. It would contain all the spoken words detected in your recording. In a real implementation, this would be generated by a speech-to-text service.\n\nThe transcript would be formatted with paragraphs and punctuation to make it easy to read. It might also include timestamps or speaker identification depending on the service used.",
  )

  // Refs
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const audioRef = useRef<HTMLAudioElement | null>(null)
  const timerRef = useRef<NodeJS.Timeout | null>(null)
  const micStreamRef = useRef<MediaStream | null>(null)
  
  // Visualization state for the recording button
  const [audioLevel, setAudioLevel] = useState<number>(0)
  const visualizationIntervalRef = useRef<NodeJS.Timeout | null>(null)
  
  // Effect to check if recording time has reached the limit
  useEffect(() => {
    if (isRecording && recordingTime >= recordingTimeLimit) {
      stopRecording();
    }
  }, [isRecording, recordingTime, recordingTimeLimit]);
  
  // Calculate time remaining in seconds
  const timeRemainingSeconds = recordingTimeLimit - recordingTime;
  
  // Format time remaining for display
  const formatTimeRemaining = () => {
    const mins = Math.floor(timeRemainingSeconds / 60);
    const secs = timeRemainingSeconds % 60;
    return `${mins}:${secs < 10 ? "0" : ""}${secs}`;
  };
  
  // Clean up resources when component unmounts
  useEffect(() => {
    return () => {
      // Stop any active recording
      if (mediaRecorderRef.current && mediaRecorderRef.current.state === "recording") {
        try {
          mediaRecorderRef.current.stop()
        } catch (e) {
          console.error("Error stopping media recorder:", e)
        }
      }
      
      // Clear timer
      if (timerRef.current) {
        clearInterval(timerRef.current)
      }
      
      // Clear visualization interval
      if (visualizationIntervalRef.current) {
        clearInterval(visualizationIntervalRef.current)
      }
      
      // Stop microphone stream
      if (micStreamRef.current) {
        try {
          micStreamRef.current.getTracks().forEach(track => {
            try {
              track.stop()
            } catch (e) {
              console.error("Error stopping track:", e)
            }
          })
        } catch (e) {
          console.error("Error stopping stream:", e)
        }
      }
      
      // Clean up audio URLs
      if (audioURL) {
        URL.revokeObjectURL(audioURL)
      }
      
      if (uploadedAudioURL) {
        URL.revokeObjectURL(uploadedAudioURL)
      }
    }
  }, [audioURL, uploadedAudioURL])
  
  // Handle audio element events
  useEffect(() => {
    const audioElement = audioRef.current
    
    if (audioElement) {
      const handleEnded = () => {
        console.log("Audio playback ended")
        setIsPlaying(false)
        setIsUploadedPlaying(false)
      }
      
      const handleError = (e: Event) => {
        console.error("Audio element error:", e)
        setIsPlaying(false)
        setIsUploadedPlaying(false)
      }
      
      audioElement.addEventListener('ended', handleEnded)
      audioElement.addEventListener('error', handleError)
      
      return () => {
        audioElement.removeEventListener('ended', handleEnded)
        audioElement.removeEventListener('error', handleError)
      }
    }
  }, [])
  
  // Format time for display
  const formatTime = (seconds: number) => {
    const mins = Math.floor(seconds / 60)
    const secs = seconds % 60
    return `${mins}:${secs < 10 ? "0" : ""}${secs}`
  }
  
  // Start recording
  const startRecording = async () => {
    try {
      console.log("Starting recording...")
      
      // Get microphone stream
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        }
      })
      
      // Store stream reference
      micStreamRef.current = stream
      
      // Reset chunks array
      audioChunksRef.current = []
      
      // Determine the supported MIME type
      const mimeType = getSupportedMimeType();
      console.log("Using MIME type:", mimeType);
      setCurrentMimeType(mimeType || 'audio/mp4');
      
      // Create media recorder with the supported MIME type
      try {
        const options = mimeType ? { mimeType } : { mimeType: 'audio/mp4' };
        mediaRecorderRef.current = new MediaRecorder(stream, options);
        console.log("MediaRecorder created successfully");
      } catch (error) {
        console.error("Error creating MediaRecorder:", error);
        // Try again without specifying a MIME type
        try {
          console.log("Trying to create MediaRecorder without MIME type");
          mediaRecorderRef.current = new MediaRecorder(stream);
          console.log("MediaRecorder created successfully without MIME type");
          // Get the actual MIME type being used
          if (mediaRecorderRef.current.mimeType) {
            setCurrentMimeType(mediaRecorderRef.current.mimeType);
            console.log("Using browser-selected MIME type:", mediaRecorderRef.current.mimeType);
          } else {
            setCurrentMimeType('audio/mp4'); // Default fallback
          }
        } catch (fallbackError) {
          console.error("Failed to create MediaRecorder even without MIME type:", fallbackError);
          alert("Your browser doesn't support audio recording. Please try a different browser.");
          // Clean up
          if (micStreamRef.current) {
            micStreamRef.current.getTracks().forEach(track => track.stop());
          }
          return;
        }
      }
      
      // Set up event handlers
      mediaRecorderRef.current.ondataavailable = (event) => {
        console.log("Data available:", event.data?.size)
        if (event.data && event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }
      
      mediaRecorderRef.current.onstop = () => {
        console.log("MediaRecorder stopped, chunks:", audioChunksRef.current.length);
        if (audioChunksRef.current.length === 0) {
          console.warn("No audio data was recorded");
          return;
        }
        try {
          console.log("Creating audio blob with MIME type: audio/mp4");
          const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/mp4' });
          const url = URL.createObjectURL(audioBlob);
          setAudioURL(url);
          console.log("Audio blob created successfully");

          // Start automatic transcription
          setAiProcessing(true);
          (async () => {
            try {
              console.log("Calling transcribeAudio with audio blob");
              const result = await transcribeAudio(audioBlob);
              let transcript = "";
              if (result.success && result.text) {
                transcript = result.text;
              } else {
                transcript = "Error: " + (result.error || "Unknown transcription error");
              }
              setTranscriptContent(transcript);
            } catch (e) {
              console.error("Error in transcription process:", e);
              setTranscriptContent("Error: " + (e instanceof Error ? e.message : String(e)));
            } finally {
              setAiProcessing(false);
            }
          })();

        } catch (error) {
          console.error("Error creating audio blob:", error);
          // Fallback logic
          try {
            const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/mp4' });
            const url = URL.createObjectURL(audioBlob);
            setAudioURL(url);
            setCurrentMimeType('audio/mp4');
            console.log("Created audio blob with fallback MIME type: audio/mp4");
          } catch (fallbackError) {
            console.error("Failed to create audio blob even with fallback:", fallbackError);
            alert("There was an error processing your recording.");
          }
        }
      }
      
      // Start recording
      mediaRecorderRef.current.start(100) // Capture in 100ms chunks
      
      // Update UI state
      setIsRecording(true)
      setRecordingTime(0)
      setIsPostRecording(false)
      
      // Start timer
      timerRef.current = setInterval(() => {
        setRecordingTime(prev => prev + 1)
      }, 1000)
      
      // Start visualization for the recording button
      startVisualization(stream)
      
      console.log("Recording started successfully")
    } catch (error) {
      console.error("Error starting recording:", error)
    }
  }
  
  // Start visualization for the recording button
  const startVisualization = (stream: MediaStream) => {
    try {
      // Clear any existing visualization interval
      if (visualizationIntervalRef.current) {
        clearInterval(visualizationIntervalRef.current)
      }
      
      // Create audio context and analyzer
      // Define the AudioContext type that includes webkitAudioContext
      type AudioContextType = typeof AudioContext
      
      // Define a type for the window with webkitAudioContext
      interface WindowWithWebkitAudio extends Window {
        webkitAudioContext?: AudioContextType;
      }
      
      const AudioContextClass: AudioContextType = 
        window.AudioContext || (window as WindowWithWebkitAudio).webkitAudioContext || null as unknown as AudioContextType;
      
      const audioContext = new AudioContextClass()
      const analyser = audioContext.createAnalyser()
      analyser.fftSize = 256
      analyser.smoothingTimeConstant = 0.3 // Make it more responsive (lower = more responsive)
      
      // Connect microphone to analyzer
      const source = audioContext.createMediaStreamSource(stream)
      source.connect(analyser)
      
      // Create data array for frequency data
      const dataArray = new Uint8Array(analyser.frequencyBinCount)
      
      // Update visualization at regular intervals
      visualizationIntervalRef.current = setInterval(() => {
        // Get frequency data
        analyser.getByteFrequencyData(dataArray)
        
        // Calculate average level with emphasis on lower frequencies
        // which are more common in speech
        let sum = 0
        let weight = 0
        for (let i = 0; i < dataArray.length; i++) {
          // Give more weight to lower frequencies (first third of the spectrum)
          const frequencyWeight = i < dataArray.length / 3 ? 3 : 1
          sum += dataArray[i] * frequencyWeight
          weight += frequencyWeight
        }
        const avg = sum / weight
        
        // Update audio level (0-100 scale) with smoother transitions
        // Amplify the effect by multiplying the raw value
        setAudioLevel(prev => {
          // Smooth transitions by blending previous and new values
          const amplifiedLevel = Math.min(100, avg * 1.5) // Amplify by 1.5x
          return prev * 0.2 + amplifiedLevel * 0.8 // 80% new value, 20% old value for more responsiveness
        })
      }, 20) // Update 50 times per second for smoother animation
      
      // Clean up when recording stops
      return () => {
        clearInterval(visualizationIntervalRef.current!)
        audioContext.close()
      }
    } catch (error) {
      console.error("Error starting visualization:", error)
    }
  }
  
  // Get dynamic gradient position based on audio level
  const getGradientPosition = () => {
    // Map audio level (0-100) to gradient position (100%-0%)
    // When audio level is high, gradient moves up (lower percentage)
    // When audio level is low, gradient moves down (higher percentage)
    const position = 100 - audioLevel;
    return `${position}%`
  }
  
  // Get button styles for recording button
  const getRecordingButtonStyles = () => {
    // Fixed size for the button
    const size = '4rem';
    
    // Create a dynamic background with gradient that moves based on audio level
    return {
      width: size,
      height: size,
      background: `linear-gradient(to top, #ef4444 ${getGradientPosition()}, #f87171 100%)`,
      boxShadow: `0 0 ${Math.max(5, audioLevel / 5)}px rgba(239, 68, 68, 0.6)`,
      transition: 'box-shadow 0.1s ease-in-out',
    };
  }
  
  // Get button styles for stop button
  const getStopButtonStyles = () => {
    // Fixed size for the button
    const size = '4rem';
    
    // Create a dynamic background with gradient that moves based on audio level
    return {
      width: size,
      height: size,
      background: `linear-gradient(to top, #b91c1c ${getGradientPosition()}, #ef4444 100%)`,
      boxShadow: `0 0 ${Math.max(5, audioLevel / 5)}px rgba(185, 28, 28, 0.6)`,
      transition: 'box-shadow 0.1s ease-in-out',
    };
  }
  
  // Stop recording
  const stopRecording = () => {
    try {
      console.log("Stopping recording...")
      
      // Stop timer
      if (timerRef.current) {
        clearInterval(timerRef.current)
        timerRef.current = null
      }
      
      // Stop visualization
      if (visualizationIntervalRef.current) {
        clearInterval(visualizationIntervalRef.current)
        visualizationIntervalRef.current = null
      }
      
      // Stop media recorder
      if (mediaRecorderRef.current && mediaRecorderRef.current.state === "recording") {
        mediaRecorderRef.current.stop()
      }
      
      // Stop microphone stream
      if (micStreamRef.current) {
        micStreamRef.current.getTracks().forEach(track => track.stop())
        micStreamRef.current = null
      }
      
      // Update UI state
      setIsRecording(false)
      setIsPostRecording(true)
      setAudioLevel(0)
      
      console.log("Recording stopped successfully")
    } catch (error) {
      console.error("Error stopping recording:", error)
      setIsRecording(false)
      setIsPostRecording(true)
    }
  }
  
  // Reset recorder to initial state
  const resetRecorder = () => {
    // Clear previous recording
    if (audioURL) {
      URL.revokeObjectURL(audioURL)
      setAudioURL(null)
    }
    
    // Reset to initial state
    setIsPostRecording(false)
    setIsRecording(false)
    setRecordingTime(0)
    setAiProcessing(false)
    setIsPlaying(false)
    
    // Stop audio playback if it's playing
    if (audioRef.current) {
      audioRef.current.pause();
      audioRef.current.currentTime = 0;
    }
  }
  
  // Handle file upload
  const handleFileUpload = (event: React.ChangeEvent<HTMLInputElement>) => {
    const files = event.target.files
    if (files && files.length > 0) {
      const file = files[0]
      
      // Check if the file is an audio file
      if (!file.type.startsWith('audio/')) {
        alert('Please upload an audio file')
        return
      }
      
      console.log("Uploaded file type:", file.type)
      setUploadedFile(file)
      
      // Create a URL for the uploaded file
      const url = URL.createObjectURL(file)
      
      // If we previously had an uploaded file, revoke its URL
      if (uploadedAudioURL) {
        URL.revokeObjectURL(uploadedAudioURL)
      }
      
      setUploadedAudioURL(url)
      setCurrentMimeType('audio/mp4') // Treat all uploads as MP4 for consistency
      
      // Reset any previous recording state
      if (isPostRecording) {
        resetRecorder()
      }
    }
  }
  
  // Toggle play recorded audio
  const togglePlayRecorded = () => {
    if (audioRef.current && audioURL) {
      if (isPlaying) {
        audioRef.current.pause()
      } else {
        // Make sure we reset the audio element before setting a new source
        audioRef.current.pause()
        audioRef.current.currentTime = 0
        audioRef.current.src = audioURL
        
        // Add error handling for playback
        const playPromise = audioRef.current.play()
        if (playPromise !== undefined) {
          playPromise.catch(error => {
            console.error("Error playing recorded audio:", error)
            alert("Error playing audio. Please try recording again.")
            setIsPlaying(false)
          })
        }
      }
      setIsPlaying(!isPlaying)
    }
  }
  
  // Toggle play uploaded audio
  const togglePlayUploaded = () => {
    if (audioRef.current && uploadedAudioURL) {
      if (isUploadedPlaying) {
        audioRef.current.pause()
      } else {
        // Make sure we reset the audio element before setting a new source
        audioRef.current.pause()
        audioRef.current.currentTime = 0
        audioRef.current.src = uploadedAudioURL
        
        // Add error handling for playback
        const playPromise = audioRef.current.play()
        if (playPromise !== undefined) {
          playPromise.catch(error => {
            console.error("Error playing uploaded audio:", error)
            alert("Error playing audio. The file format may not be supported.")
            setIsUploadedPlaying(false)
          })
        }
      }
      setIsUploadedPlaying(!isUploadedPlaying)
    }
  }
  
  // Process with AI
  const processWithAI = () => {
    setAiProcessing(true);
    const newId = new Date().getTime();
    
    // Only add to processedResults for summarize and analyze actions, not for transcripts
    if (selectedAiAction !== "transcribe") {
      // Add a new result card with generating state
      setProcessedResults(prev => [
        ...prev,
        { 
          id: newId, 
          type: selectedAiAction, 
          content: "", 
          title: "", 
          generating: true, 
          expanded: false,
          date: new Date().toISOString() // Add date field
        }
      ]);
    }

    // Simulate AI processing
    setTimeout(() => {
      let result = "";
      let title = ""; // Add dynamic title
      
      if (selectedAiAction === "summarize") {
        // Array of possible summary contents
        const summaryContents = [
          "The speaker discussed their recent project involving audio transcription and analysis. They mentioned challenges with accuracy in noisy environments and outlined three potential solutions: improved noise filtering, speaker separation algorithms, and context-aware transcription. They plan to implement these improvements over the next two weeks and test with real-world recordings.",
          
          "This meeting covered quarterly goals for the marketing team. Key points included increasing social media engagement by 15%, launching the new product campaign by August, and improving customer feedback scores. Team members were assigned specific responsibilities, with progress reviews scheduled bi-weekly.",
          
          "The recording contains notes about a new business idea for a mobile app that helps people track and reduce their carbon footprint. The speaker outlined features including transportation emissions calculator, household energy usage tracking, and personalized recommendations. They identified potential partners and a rough timeline for development.",
          
          "The speaker recorded thoughts about improving their daily routine. Main points included: starting the day earlier (5:30 AM), incorporating 20 minutes of meditation, planning meals in advance, and dedicating 1 hour to learning new skills. They noted that consistency would be the biggest challenge."
        ];
        
        // Randomly select content
        result = summaryContents[Math.floor(Math.random() * summaryContents.length)];
        
        // Array of possible summary titles
        const summaryTitles = [
          "Key Insights from Your Recording",
          "Main Points from Your Conversation",
          "Essential Takeaways from Discussion",
          "Your Meeting Summarized",
          "Core Ideas from Your Audio",
          "Conversation Highlights",
          "Important Discussion Points",
          "Quick Summary of Your Recording"
        ];
        
        // Randomly select a title
        title = summaryTitles[Math.floor(Math.random() * summaryTitles.length)];
        
      } else if (selectedAiAction === "analyze") {
        // Array of possible analysis contents
        const analysisContents = [
          "This recording demonstrates a structured approach to problem-solving. The speaker presents ideas logically, with clear transitions between topics. The tone is primarily analytical (78%) with moments of enthusiasm (22%). Key themes include innovation, efficiency, and practical implementation. The speaker uses technical terminology appropriately, suggesting expertise in the subject matter. Recommendations: Consider adding more concrete examples to illustrate abstract concepts.",
          
          "The conversation exhibits a collaborative dynamic between participants. Speaker A contributes 60% of the content, while Speaker B provides 40%. The discussion shows a pattern of proposal → questioning → refinement, indicating effective teamwork. Emotional analysis shows primarily neutral tones (65%) with periods of excitement (25%) and concern (10%). The pace increases when discussing deadlines, suggesting time pressure is a significant factor.",
          
          "This recording contains a personal reflection with strong emotional elements. The content follows a narrative structure, beginning with a challenge, exploring options, and concluding with a decision. Sentiment analysis shows a progression from uncertainty (beginning) to confidence (end). The speaker uses first-person perspective consistently and employs metaphorical language to describe complex feelings. The audio contains several significant pauses (3-5 seconds) during critical decision points.",
          
          "The presentation in this recording follows a classic persuasive structure. The speaker establishes credibility early, presents a problem, offers a solution, and concludes with a call to action. Language analysis shows effective use of inclusive pronouns ('we', 'our') to build rapport. The argument is supported primarily by statistical evidence (65%) and case studies (35%). The speaker effectively anticipates and addresses potential counterarguments."
        ];
        
        // Randomly select content
        result = analysisContents[Math.floor(Math.random() * analysisContents.length)];
        
        // Array of possible analysis titles
        const analysisTitles = [
          "Detailed Analysis of Discussion Points",
          "In-depth Examination of Your Recording",
          "Comprehensive Analysis of Conversation",
          "Sentiment and Content Analysis",
          "Analytical Breakdown of Key Themes",
          "Contextual Analysis of Your Discussion",
          "Thematic Analysis of Recording",
          "Conversational Patterns and Insights"
        ];
        
        // Randomly select a title
        title = analysisTitles[Math.floor(Math.random() * analysisTitles.length)];
      }

      if (selectedAiAction !== "transcribe") {
        // Update the corresponding result card
        setProcessedResults(prev =>
          prev.map(entry => entry.id === newId ? { ...entry, content: result, title, generating: false } : entry)
        );
        // Don't collapse the UI for non-transcript actions
      } else {
        // For transcripts, update the transcriptContent state
        // Generate a more realistic transcript that matches the summaries/analyses
        const transcripts = [
          "So I've been working on this audio transcription project for the past few weeks. The main challenge we're facing is accuracy in noisy environments. I've identified three potential solutions we could implement. First, we could improve our noise filtering algorithms. Second, we could implement better speaker separation. And third, we could develop more context-aware transcription that uses the surrounding text to infer words that might be unclear. I'm planning to implement these improvements over the next two weeks and then test them with some real-world recordings to see how much we can improve the accuracy.",
          
          "Alright team, let's go over our quarterly goals for marketing. First, we need to increase our social media engagement by at least 15%. Sarah, you'll be leading that initiative. Second, we need to launch the new product campaign by August. That gives us about two months to prepare. John, you'll be working with the product team on that. And finally, we need to improve our customer feedback scores. Our current NPS is at 32, and we want to get it to at least 40 by the end of the quarter. Maria, that's your area. Let's plan to review progress bi-weekly to make sure we're on track.",
          
          "I've been thinking about this mobile app idea that helps people track and reduce their carbon footprint. The main features would include a transportation emissions calculator that uses GPS data to estimate emissions from different modes of transport. We'd also have a household energy usage tracker that connects to smart home devices or lets users input their utility bills. And then we'd provide personalized recommendations based on their data. I've identified some potential partners including local utility companies and environmental nonprofits. I think we could have a prototype ready in about three months if we start now.",
          
          "Notes to self about improving my daily routine. I need to start waking up earlier, maybe 5:30 AM instead of 7. That would give me time for meditation, which I want to do for at least 20 minutes each morning. I also need to get better at planning my meals in advance to avoid unhealthy takeout. And I want to dedicate at least one hour each day to learning new skills, probably in the evening when I'm usually just scrolling through social media. The biggest challenge will be consistency. I always start these things and then give up after a week or two."
        ];
        
        // Randomly select a transcript
        setTranscriptContent(transcripts[Math.floor(Math.random() * transcripts.length)]);
      }
      
      setAiProcessing(false);
    }, 2000);
  };

  // Helper function to determine the supported MIME type
  const getSupportedMimeType = () => {
    // Only support MP4 format as requested
    if (MediaRecorder.isTypeSupported('audio/mp4')) {
      console.log('Browser supports recording in audio/mp4 format');
      return 'audio/mp4';
    }
    
    console.warn('MP4 format not supported. Using default browser implementation.');
    // Fallback to default (browser will choose)
    return '';
  };

  // Helper function to get file extension from MIME type
  const getFileExtensionFromMimeType = (mimeType: string): string => {
    // Default to mp4 as requested
    if (!mimeType || mimeType.includes('mp4')) return 'mp4';
    if (mimeType.includes('webm')) return 'webm';
    if (mimeType.includes('ogg')) return 'ogg';
    if (mimeType.includes('wav')) return 'wav';
    if (mimeType.includes('mp3') || mimeType.includes('mpeg')) return 'mp3';
    if (mimeType.includes('aac')) return 'aac';
    
    // Default fallback
    return 'mp4';
  };

  // Call onResultsChange when processedResults changes
  useEffect(() => {
    if (onResultsChange) {
      onResultsChange(processedResults);
    }
  }, [processedResults, onResultsChange]);

  // Function to toggle expanded state for a card
  const toggleCardExpanded = (id: number) => {
    setProcessedResults(prev => 
      prev.map(result => 
        result.id === id ? { ...result, expanded: !result.expanded } : result
      )
    );
  };

  return (
    <>
      {processedResults.length > 0 && (
        <div className="fixed top-32 left-0 right-0 px-8 z-[60] overflow-y-auto" style={{ maxHeight: 'calc(100vh - 200px)' }}>
          <div className="grid grid-cols-1 sm:grid-cols-1 md:grid-cols-2 lg:grid-cols-3 xl:grid-cols-4 gap-6 pb-32">
            {processedResults.filter(result => result.type !== "transcribe").map(result => (
              <div key={result.id} className={`${result.expanded ? 'col-span-full' : ''} transition-all duration-300`}>
                <div 
                  className={`w-full ${result.expanded ? 'min-h-[400px]' : 'h-[230px]'} bg-white/10 backdrop-blur-md rounded-lg border border-white/20 shadow-lg overflow-hidden flex flex-col ${result.generating ? "opacity-50" : "opacity-100"}`}
                >
                  <div className="p-3 border-b border-white/20 bg-white/5 flex justify-between items-center">
                    <h4 className="font-medium text-white text-base truncate max-w-[85%]">
                      {result.generating ? 
                        (result.type === "summarize" ? "Generating Summary..." : "Generating Analysis...") : 
                        (result.title || (result.type === "summarize" ? "Summary" : "Analysis"))}
                    </h4>
                    <Button
                      variant="ghost"
                      size="icon"
                      className="h-6 w-6 rounded-full bg-white/10 hover:bg-white/20 text-white/70 hover:text-white"
                      onClick={() => toggleCardExpanded(result.id)}
                      disabled={result.generating}
                    >
                      {result.expanded ? <Minimize2 className="h-3 w-3" /> : <Maximize2 className="h-3 w-3" />}
                    </Button>
                  </div>
                  <div className={`p-3 flex-1 overflow-y-auto text-sm text-white/90 ${result.expanded ? 'flex flex-col' : ''}`}>
                    {result.generating ? (
                      <div className="flex items-center justify-center h-full">
                        <div className="animate-pulse">Generating...</div>
                      </div>
                    ) : (
                      <>
                        <div className={`whitespace-pre-line ${result.expanded ? 'flex-1' : 'line-clamp-7'}`}>
                          {result.content.includes("This is a") ? 
                            result.content.replace(/^This is a (summary|analysis) of the audio recording\.\s+/, "") : 
                            result.content}
                        </div>
                        
                        {result.expanded && (
                          <div className="mt-4">
                            <div 
                              className="p-3 bg-white/5 rounded-lg mb-2 flex justify-between items-center cursor-pointer"
                              onClick={() => {
                                const transcriptEl = document.getElementById(`transcript-${result.id}`);
                                if (transcriptEl) {
                                  transcriptEl.classList.toggle('hidden');
                                }
                              }}
                            >
                              <h5 className="font-medium text-white/90 text-sm">Original Transcript</h5>
                              <ChevronDown className="h-4 w-4 text-white/70" />
                            </div>
                            <div id={`transcript-${result.id}`} className="p-3 bg-white/5 rounded-lg hidden">
                              <div className="whitespace-pre-line text-white/80 text-xs">
                                {transcriptContent}
                              </div>
                            </div>
                          </div>
                        )}
                        
                        {!result.expanded && (
                          <div className="text-xs text-white/50 absolute bottom-3 left-3 pt-5">
                            <span>{new Date(result.date || Date.now()).toLocaleDateString('en-US', { 
                              month: 'short', 
                              day: 'numeric',
                              hour: '2-digit',
                              minute: '2-digit'
                            })}</span>
                          </div>
                        )}
                      </>
                    )}
                  </div>
                  
                  {result.expanded && (
                    <div className="px-4 py-3 border-t border-white/10 bg-white/5 text-xs text-white/60 flex justify-between">
                      <span>{new Date(result.date || Date.now()).toLocaleDateString('en-US', { 
                        year: 'numeric',
                        month: 'short', 
                        day: 'numeric',
                        hour: '2-digit',
                        minute: '2-digit'
                      })}</span>
                      <span>{result.type === "summarize" ? "Summary" : "Analysis"}</span>
                    </div>
                  )}
                </div>
              </div>
            ))}
          </div>
        </div>
      )}
      <Card className={`fixed bottom-4 left-1/2 transform -translate-x-1/2 ${isMinimized ? 'w-auto' : 'w-full max-w-md'} overflow-hidden bg-white/10 backdrop-blur-md border-0 shadow-xl z-[100] transition-all duration-300`}>
        <div className="absolute top-2 right-2 z-10">
          <Button
            variant="ghost"
            size="icon"
            className="h-6 w-6 rounded-full bg-white/10 hover:bg-white/20 text-white/70 hover:text-white"
            onClick={() => setIsMinimized(!isMinimized)}
          >
            {isMinimized ? <Maximize2 className="h-3 w-3" /> : <X className="h-3 w-3" />}
          </Button>
        </div>
        {isMinimized ? (
          <div className="p-3 flex items-center space-x-3">
            <Button
              variant="default"
              style={getRecordingButtonStyles()}
              className="rounded-full p-0 flex items-center justify-center h-10 w-10"
              onClick={() => {
                if (isRecording) {
                  stopRecording();
                } else {
                  setIsMinimized(false);
                  startRecording();
                }
              }}
            >
              {isRecording ? <Square className="h-4 w-4" /> : <Mic className="h-4 w-4" />}
            </Button>
            <span className="text-xs text-white/80 font-medium">
              {isRecording ? formatTimeRemaining() : "Record Audio"}
            </span>
          </div>
        ) : (
        <CardContent className="p-5">
          <Tabs defaultValue="record" className="w-full">
            <TabsList className="grid w-full grid-cols-2 mb-6 bg-white/10 p-1 rounded-lg">
              <TabsTrigger
                value="record"
                className="data-[state=active]:bg-white/20 data-[state=active]:text-white data-[state=active]:shadow-md text-white/70"
              >
                Record
              </TabsTrigger>
              <TabsTrigger
                value="upload"
                className="data-[state=active]:bg-white/20 data-[state=active]:text-white data-[state=active]:shadow-md text-white/70"
              >
                Upload
              </TabsTrigger>
            </TabsList>

            <TabsContent value="record" className="space-y-5">
              {/* Recording controls */}
              <div className="space-y-4">
                {/* Timer display - show time remaining when recording, or elapsed time after recording */}
                {isRecording ? (
                  <div className="text-center">
                    <div className="text-3xl font-mono font-bold text-white">
                      {formatTimeRemaining()}
                    </div>
                    <div className="text-xs text-white/70 mt-1">
                      {isAuthenticated 
                        ? "Premium: 10 minute recording limit" 
                        : "Free: 5 minute recording limit"}
                    </div>
                  </div>
                ) : isPostRecording && (
                  <div className="text-center">
                    <span className="text-3xl font-mono font-bold text-white">
                      {formatTime(recordingTime)}
                    </span>
                  </div>
                )}

                {/* Control buttons */}
                <div className="flex justify-center gap-4">
                  {isRecording ? (
                    <Button
                      variant="destructive"
                      style={getStopButtonStyles()}
                      className="rounded-full p-0 flex items-center justify-center"
                      onClick={stopRecording}
                    >
                      <Square className="h-6 w-6" />
                    </Button>
                  ) : isPostRecording ? (
                    <div className="flex gap-3">
                      <Button
                        variant="outline"
                        size="icon"
                        className="rounded-full w-12 h-12 border-white/20 bg-white/10 text-white hover:bg-white/20 hover:text-white"
                        onClick={resetRecorder}
                      >
                        <Trash2 className="h-5 w-5" />
                      </Button>
                      <Button
                        variant="outline"
                        size="icon"
                        className="rounded-full w-12 h-12 border-white/20 bg-white/10 text-white hover:bg-white/20 hover:text-white"
                        onClick={togglePlayRecorded}
                      >
                        {isPlaying ? <Pause className="h-5 w-5" /> : <Play className="h-5 w-5" />}
                      </Button>
                      <Button
                        variant="outline"
                        size="icon"
                        className="rounded-full w-12 h-12 border-white/20 bg-white/10 text-white hover:bg-white/20 hover:text-white"
                        onClick={resetRecorder}
                      >
                        <PlusCircle className="h-5 w-5" />
                      </Button>
                    </div>
                  ) : (
                    <div className="flex flex-col items-center">
                      <Button
                        variant="default"
                        style={getRecordingButtonStyles()}
                        className="rounded-full p-0 flex items-center justify-center mb-2"
                        onClick={startRecording}
                      >
                        <Mic className="h-6 w-6" />
                      </Button>
                      <span className="text-xs text-white/60">Tap to record</span>
                    </div>
                  )}
                </div>
              </div>

              {/* AI Processing section - only show after recording */}
              {isPostRecording && audioURL && (
                <div className="space-y-4 mt-6 pt-6 border-t border-white/20">
                  <Select
                    value={selectedAiAction}
                    onValueChange={setSelectedAiAction}
                  >
                    <SelectTrigger className="w-full h-12 text-white text-base bg-white/10 border-white/20 focus:ring-white/30">
                      <SelectValue placeholder="Select AI action" />
                      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round" className="ml-2">
                        <path d="m6 9 6 6 6-6"/>
                      </svg>
                    </SelectTrigger>
                    <SelectContent className="bg-slate-800 border-white/20 text-white">
                      <SelectItem value="summarize" className="text-base focus:bg-white/10 focus:text-white">Summarize</SelectItem>
                      <SelectItem value="analyze" className="text-base focus:bg-white/10 focus:text-white">Analyze</SelectItem>
                    </SelectContent>
                  </Select>

                  <Button
                    variant="default"
                    className="w-full bg-gradient-to-r from-indigo-500 to-purple-600 hover:from-indigo-600 hover:to-purple-700 text-white h-12 text-base"
                    disabled={aiProcessing}
                    onClick={processWithAI}
                  >
                    {aiProcessing ? "Processing..." : `${selectedAiAction.charAt(0).toUpperCase() + selectedAiAction.slice(1)} Audio`}
                  </Button>
                  
                  {/* Display transcript in the recorder card */}
                  <div className="mt-4 p-4 bg-white/10 rounded-lg border border-white/20 text-sm text-white/90">
                    <h4 className="font-medium mb-2 text-white">Transcript</h4>
                    <div className="whitespace-pre-line max-h-60 overflow-y-auto">
                      {aiProcessing ? (
                        <div className="flex items-center justify-center py-4">
                          <div className="animate-pulse">Generating transcript...</div>
                        </div>
                      ) : (
                        transcriptContent
                      )}
                    </div>
                  </div>
                </div>
              )}
            </TabsContent>

            <TabsContent value="upload" className="space-y-5">
              {!uploadedFile ? (
                <div className="flex flex-col items-center justify-center p-6 border-2 border-dashed border-white/20 rounded-lg bg-white/5">
                  <Upload className="h-8 w-8 text-white/50 mb-2" />
                  <p className="text-sm text-white/70 mb-4 text-center">
                    Upload an audio file to process with AI
                  </p>
                  <input
                    type="file"
                    id="audio-upload"
                    accept="audio/*"
                    className="hidden"
                    onChange={handleFileUpload}
                  />
                  <label
                    htmlFor="audio-upload"
                    className="inline-flex items-center justify-center px-4 py-2 bg-white/20 text-white text-sm font-medium rounded-md hover:bg-white/30 cursor-pointer transition-colors"
                  >
                    Select Audio File
                  </label>
                </div>
              ) : (
                <div className="p-4 bg-white/10 rounded-lg border border-white/20">
                  <div className="flex items-center justify-between mb-4">
                    <div>
                      <h3 className="font-medium text-white">{uploadedFile.name}</h3>
                      <p className="text-xs text-white/60">
                        {(uploadedFile.size / 1024 / 1024).toFixed(2)} MB
                      </p>
                    </div>
                    <Button
                      variant="outline"
                      size="icon"
                      className="h-8 w-8 border-white/20 bg-white/10 text-white hover:bg-white/20 hover:text-white"
                      onClick={togglePlayUploaded}
                    >
                      {isUploadedPlaying ? <Pause className="h-4 w-4" /> : <Play className="h-4 w-4" />}
                    </Button>
                  </div>

                  <div className="space-y-4">
                    <Select
                      value={selectedAiAction}
                      onValueChange={setSelectedAiAction}
                    >
                      <SelectTrigger className="w-full h-12 text-white text-base bg-white/10 border-white/20 focus:ring-white/30">
                        <SelectValue placeholder="Select AI action" />
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round" className="ml-2">
                          <path d="m6 9 6 6 6-6"/>
                        </svg>
                      </SelectTrigger>
                      <SelectContent className="bg-slate-800 border-white/20 text-white">
                        <SelectItem value="summarize" className="text-base focus:bg-white/10 focus:text-white">Summarize</SelectItem>
                        <SelectItem value="analyze" className="text-base focus:bg-white/10 focus:text-white">Analyze</SelectItem>
                      </SelectContent>
                    </Select>

                    <Button
                      variant="default"
                      className="w-full bg-gradient-to-r from-indigo-500 to-purple-600 hover:from-indigo-600 hover:to-purple-700 text-white h-12 text-base"
                      disabled={aiProcessing}
                      onClick={processWithAI}
                    >
                      {aiProcessing ? "Processing..." : `${selectedAiAction.charAt(0).toUpperCase() + selectedAiAction.slice(1)} Audio`}
                    </Button>

                    {/* Display transcript in the upload tab */}
                    <div className="mt-4 p-4 bg-white/10 rounded-lg border border-white/20 text-sm text-white/90">
                      <h4 className="font-medium mb-2 text-white">Transcript</h4>
                      <div className="whitespace-pre-line max-h-60 overflow-y-auto">
                        {aiProcessing ? (
                          <div className="flex items-center justify-center py-4">
                            <div className="animate-pulse">Generating transcript...</div>
                          </div>
                        ) : (
                          transcriptContent
                        )}
                      </div>
                    </div>
                  </div>
                </div>
              )}
            </TabsContent>
          </Tabs>
        </CardContent>
        )}
        {/* Only show footer with save button after recording and when not minimized */}
        {isPostRecording && audioURL && !isMinimized && (
          <CardFooter className="px-5 py-3 border-t border-white/20 bg-white/5 text-xs text-white/60">
            <div className="w-full flex justify-between items-center">
              <span>Audio Recorder</span>
              <a
                href="#"
                className="text-white/80 hover:text-white hover:underline transition-colors"
                onClick={(e) => {
                  e.preventDefault()
                  if (audioURL) {
                    const a = document.createElement("a")
                    a.href = audioURL
                    const extension = getFileExtensionFromMimeType(currentMimeType);
                    a.download = `recording.${extension}`
                    a.click()
                  }
                }}
              >
                <Save className="h-4 w-4 inline-block mr-1" />
                Save Recording
              </a>
            </div>
          </CardFooter>
        )}
        {/* Audio element for playback */}
        <audio 
          ref={audioRef} 
          className="hidden" 
          controls={false}
          preload="auto"
          onError={(e) => console.error("Audio error:", e)}
        />
      </Card>
    </>
  )
}

