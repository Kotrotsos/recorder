"use client"

import type React from "react"

import { useState, useRef, useEffect, useCallback } from "react"
import { Upload, Mic, Square, Play, Pause, Save, Trash2, Volume2, VolumeX, PlusCircle } from "lucide-react"
import { Button } from "@/components/ui/button"
import { Card, CardContent, CardFooter } from "@/components/ui/card"
import { Tabs, TabsContent, TabsList, TabsTrigger } from "@/components/ui/tabs"
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from "@/components/ui/select"
import { Separator } from "@/components/ui/separator"

export default function AudioRecorder() {
  const [isRecording, setIsRecording] = useState(false)
  const [audioURL, setAudioURL] = useState<string | null>(null)
  const [recordingTime, setRecordingTime] = useState(0)
  const [audioLevel, setAudioLevel] = useState(0)
  const [isPlaying, setIsPlaying] = useState(false) // Fixed: Initialize with false instead of circular reference
  const [isMuted, setIsMuted] = useState(false)
  const [uploadedFile, setUploadedFile] = useState<File | null>(null)
  const [uploadedAudioURL, setUploadedAudioURL] = useState<string | null>(null)
  const [isAudioContextInitialized, setIsAudioContextInitialized] = useState(false)

  const [selectedAiAction, setSelectedAiAction] = useState<string>("transcribe")
  const [aiProcessing, setAiProcessing] = useState(false)
  const [aiResult, setAiResult] = useState<string | null>(null)

  // Add a new state to track if we're in post-recording mode
  const [isPostRecording, setIsPostRecording] = useState(false)

  // Add a new state for the transcript
  const [transcriptContent, setTranscriptContent] = useState<string>(
    "This is a simulated transcript of your audio recording. It would contain all the spoken words detected in your recording. In a real implementation, this would be generated by a speech-to-text service.\n\nThe transcript would be formatted with paragraphs and punctuation to make it easy to read. It might also include timestamps or speaker identification depending on the service used.",
  )

  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyserRef = useRef<AnalyserNode | null>(null)
  const gainNodeRef = useRef<GainNode | null>(null)
  const micStreamRef = useRef<MediaStream | null>(null)
  const recordingStreamRef = useRef<MediaStream | null>(null)
  const audioRef = useRef<HTMLAudioElement | null>(null)
  const animationFrameRef = useRef<number | null>(null)
  const timerRef = useRef<NodeJS.Timeout | null>(null)

  // Initialize audio context and analyzer for visualization only
  const setupAudioContext = useCallback(async () => {
    try {
      console.log("Setting up audio context for visualization")

      // If we already have a visualization running, clean it up first
      if (micStreamRef.current) {
        console.log("Stopping existing mic stream")
        micStreamRef.current.getTracks().forEach((track) => track.stop())
        micStreamRef.current = null
      }

      if (animationFrameRef.current) {
        console.log("Canceling existing animation frame")
        cancelAnimationFrame(animationFrameRef.current)
        animationFrameRef.current = null
      }

      // Close existing audio context if it exists
      if (audioContextRef.current && audioContextRef.current.state !== "closed") {
        try {
          console.log("Closing existing audio context")
          await audioContextRef.current.close()
          audioContextRef.current = null
        } catch (error) {
          console.warn("Error closing AudioContext:", error)
        }
      }

      // Get a fresh microphone stream for visualization
      console.log("Getting new microphone stream for visualization")
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      micStreamRef.current = stream

      // Create new audio context
      console.log("Creating new audio context")
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)()
      audioContextRef.current = audioContext

      // Ensure the audio context is running
      if (audioContext.state === "suspended") {
        console.log("Resuming audio context")
        await audioContext.resume()
      }

      // Create gain node for muting
      console.log("Creating gain node")
      const gainNode = audioContext.createGain()
      gainNode.gain.value = isMuted ? 0.0 : 1.0
      gainNodeRef.current = gainNode

      // Create and configure analyzer for frequency analysis
      console.log("Creating analyzer node")
      const analyser = audioContext.createAnalyser()
      analyser.fftSize = 2048
      analyser.smoothingTimeConstant = 0.85
      analyserRef.current = analyser

      // Connect microphone to gain node to analyzer
      console.log("Connecting audio nodes")
      const source = audioContext.createMediaStreamSource(stream)
      source.connect(gainNode)
      gainNode.connect(analyser)

      // Start the audio level monitoring
      console.log("Starting animation frame for visualization")
      animationFrameRef.current = requestAnimationFrame(updateAudioLevel)

      setIsAudioContextInitialized(true)
      console.log("Audio context setup complete")
      return true
    } catch (error) {
      console.error("Error setting up audio context:", error)
      return false
    }
  }, [isMuted])

  // Toggle mute state for visualization
  const toggleMute = () => {
    if (gainNodeRef.current) {
      const newMuteState = !isMuted
      setIsMuted(newMuteState)

      // Set gain to 0 when muted, 1 when unmuted
      gainNodeRef.current.gain.value = newMuteState ? 0 : 1
    }
  }

  // Update VU meter with frequency data
  const updateAudioLevel = () => {
    if (!analyserRef.current || !audioContextRef.current) {
      console.log("Analyzer or audio context not available, rescheduling animation frame")
      animationFrameRef.current = requestAnimationFrame(updateAudioLevel)
      return
    }

    // Check if audio context is running
    if (audioContextRef.current.state === "suspended") {
      console.log("Audio context suspended, attempting to resume")
      audioContextRef.current.resume().then(() => {
        console.log("Audio context resumed")
        animationFrameRef.current = requestAnimationFrame(updateAudioLevel)
      })
      return
    }

    try {
      const bufferLength = analyserRef.current.frequencyBinCount
      const dataArray = new Uint8Array(bufferLength)
      analyserRef.current.getByteFrequencyData(dataArray)

      // Force a re-render to update the visualization
      setAudioLevel((prev) => (prev === 0 ? 0.1 : 0))

      // Continue the animation loop
      animationFrameRef.current = requestAnimationFrame(updateAudioLevel)
    } catch (error) {
      console.error("Error in updateAudioLevel:", error)
      // Try to recover by restarting the visualization
      restartVisualization()
    }
  }

  // Add this new function to restart the visualization
  const restartVisualization = useCallback(async () => {
    console.log("Restarting visualization")

    // Clean up existing resources
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current)
      animationFrameRef.current = null
    }

    // Set up a fresh audio context and visualization
    await setupAudioContext()
  }, [setupAudioContext])

  // Get a separate stream for recording
  const getRecordingStream = async () => {
    try {
      // Get a fresh stream for recording to avoid any processing conflicts
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        },
      })

      recordingStreamRef.current = stream
      return stream
    } catch (error) {
      console.error("Error getting recording stream:", error)
      return null
    }
  }

  // Start recording
  const startRecording = async () => {
    try {
      // Initialize audio context for visualization if not already done
      if (!isAudioContextInitialized) {
        const success = await setupAudioContext()
        if (!success) {
          console.error("Failed to initialize audio context")
          return
        }
      }

      // Get a separate stream for recording
      const recordingStream = await getRecordingStream()
      if (!recordingStream) {
        console.error("Failed to get recording stream")
        return
      }

      // Reset chunks array
      audioChunksRef.current = []

      // Create a new MediaRecorder with minimal options
      try {
        mediaRecorderRef.current = new MediaRecorder(recordingStream)
      } catch (e) {
        console.error("Error creating MediaRecorder:", e)
        return
      }

      // Set up event handlers
      mediaRecorderRef.current.ondataavailable = (event) => {
        if (event.data && event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }

      mediaRecorderRef.current.onerror = (event) => {
        console.error("MediaRecorder error:", event)
        stopRecording()
      }

      mediaRecorderRef.current.onstop = () => {
        if (audioChunksRef.current.length === 0) {
          console.warn("No audio data was recorded")
          return
        }

        // Create blob and URL
        const audioBlob = new Blob(audioChunksRef.current, { type: "audio/webm" })
        const url = URL.createObjectURL(audioBlob)
        setAudioURL(url)
      }

      // Start recording
      mediaRecorderRef.current.start(100) // Capture in 100ms chunks
      setIsRecording(true)
      setRecordingTime(0)

      // Start timer
      timerRef.current = setInterval(() => {
        setRecordingTime((prev) => prev + 1)
      }, 1000)

      // Make sure we're updating the audio level for visualization
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current)
      }
      animationFrameRef.current = requestAnimationFrame(updateAudioLevel)
    } catch (error) {
      console.error("Error starting recording:", error)
    }
  }

  // Modify the stopRecording function to set isPostRecording to true
  const stopRecording = () => {
    try {
      if (mediaRecorderRef.current && isRecording) {
        console.log("Stopping recording")

        // Only call stop if the state is "recording"
        if (mediaRecorderRef.current.state === "recording") {
          mediaRecorderRef.current.stop()
        }

        // Stop only the recording stream, not the visualization stream
        if (recordingStreamRef.current) {
          console.log("Stopping recording stream")
          recordingStreamRef.current.getTracks().forEach((track) => track.stop())
          recordingStreamRef.current = null
        }

        setIsRecording(false)
        // Set post-recording mode
        setIsPostRecording(true)

        // Stop timer
        if (timerRef.current) {
          clearInterval(timerRef.current)
          timerRef.current = null
        }

        // Reset mute state when stopping recording
        setIsMuted(false)
        if (gainNodeRef.current) {
          gainNodeRef.current.gain.value = 1
        }
      }
    } catch (error) {
      console.error("Error stopping recording:", error)
      setIsRecording(false)
      setIsPostRecording(true)
    }
  }

  // Add a new function to start a new recording
  const startNewRecording = () => {
    // Clear previous recording
    if (audioURL) {
      URL.revokeObjectURL(audioURL)
      setAudioURL(null)
    }
    setRecordingTime(0)
    setAiResult(null)
    setIsPostRecording(false)

    // Restart visualization
    restartVisualization()
  }

  // Modify the clearRecording function to also reset the post-recording state
  const clearRecording = () => {
    if (audioURL) {
      URL.revokeObjectURL(audioURL)
      setAudioURL(null)
    }
    setRecordingTime(0)
    setAiResult(null)
    setIsPostRecording(false)
  }

  // Handle file upload
  const handleFileUpload = (event: React.ChangeEvent<HTMLInputElement>) => {
    const file = event.target.files?.[0]
    if (file && file.type.startsWith("audio/")) {
      setUploadedFile(file)

      // Create URL for the uploaded audio
      if (uploadedAudioURL) {
        URL.revokeObjectURL(uploadedAudioURL)
      }

      const url = URL.createObjectURL(file)
      setUploadedAudioURL(url)
    }
  }

  // Toggle play/pause for recorded audio
  const togglePlayRecorded = () => {
    if (audioRef.current) {
      if (isPlaying) {
        audioRef.current.pause()
      } else {
        audioRef.current.src = audioURL as string
        audioRef.current.play()
      }
      setIsPlaying(!isPlaying)
    }
  }

  // Toggle play/pause for uploaded audio
  const togglePlayUploaded = () => {
    if (audioRef.current) {
      if (isPlaying) {
        audioRef.current.pause()
      } else {
        audioRef.current.src = uploadedAudioURL as string
        audioRef.current.play()
      }
      setIsPlaying(!isPlaying)
    }
  }

  // Format time as mm:ss
  const formatTime = (seconds: number) => {
    const mins = Math.floor(seconds / 60)
    const secs = seconds % 60
    return `${mins.toString().padStart(2, "0")}:${secs.toString().padStart(2, "0")}`
  }

  // Clean up on unmount
  useEffect(() => {
    return () => {
      if (timerRef.current) {
        clearInterval(timerRef.current)
      }

      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current)
      }

      if (micStreamRef.current) {
        micStreamRef.current.getTracks().forEach((track) => track.stop())
      }

      if (recordingStreamRef.current) {
        recordingStreamRef.current.getTracks().forEach((track) => track.stop())
      }

      if (audioURL) {
        URL.revokeObjectURL(audioURL)
      }

      if (uploadedAudioURL) {
        URL.revokeObjectURL(uploadedAudioURL)
      }

      // Safely close the AudioContext
      if (audioContextRef.current && audioContextRef.current.state !== "closed") {
        try {
          audioContextRef.current.close()
        } catch (error) {
          console.warn("Error closing AudioContext:", error)
        }
      }
    }
  }, [audioURL, uploadedAudioURL])

  // Handle audio ended event
  useEffect(() => {
    const handleAudioEnded = () => {
      setIsPlaying(false)
    }

    if (audioRef.current) {
      audioRef.current.addEventListener("ended", handleAudioEnded)
    }

    return () => {
      if (audioRef.current) {
        audioRef.current.removeEventListener("ended", handleAudioEnded)
      }
    }
  }, [])

  // Initialize audio context when component mounts
  useEffect(() => {
    const initAudio = async () => {
      await setupAudioContext()
    }

    initAudio()
  }, [setupAudioContext])

  // Process audio with AI
  const processWithAI = () => {
    setAiProcessing(true)
    setAiResult(null)

    // Simulate AI processing with a timeout
    setTimeout(() => {
      let result = ""

      switch (selectedAiAction) {
        case "transcribe":
          result =
            "This is a simulated transcription of your audio recording. In a real implementation, this would use a speech-to-text AI service to convert your audio to text."
          break
        case "summarize":
          result =
            "Summary: This audio appears to contain a person speaking about an important topic. The key points include several main ideas that were articulated clearly."
          break
        case "sentiment":
          result =
            "Sentiment Analysis: Positive (78%)\nThe tone of this recording appears mostly positive with moderate confidence."
          break
        case "translate":
          result =
            "Translation (to Spanish):\nEsto es una traducción simulada de su grabación de audio. En una implementación real, esto utilizaría un servicio de IA de traducción."
          break
        case "enhance":
          result =
            "Audio has been enhanced. Background noise reduced by 40%, voice clarity improved by 35%. (This is a simulation - no actual processing occurred)"
          break
        default:
          result = "Processing complete."
      }

      setAiResult(result)
      setAiProcessing(false)
    }, 2000) // Simulate 2 second processing time
  }

  // Get color for frequency bar based on index and value
  const getBarColor = (index: number, value: number) => {
    // Create a more vibrant color palette
    if (value < 20) return "rgba(74, 222, 128, 0.8)" // Light green
    if (index < 8) return "rgba(52, 211, 153, 0.9)" // Teal
    if (index < 16) return "rgba(45, 212, 191, 0.9)" // Cyan
    if (index < 24) return "rgba(56, 189, 248, 0.9)" // Light blue
    return "rgba(168, 85, 247, 0.9)" // Purple
  }

  // Add a useEffect to restart visualization when switching tabs
  const handleVisibilityChange = useCallback(() => {
    if (document.visibilityState === "visible" && !isRecording) {
      console.log("Page became visible, restarting visualization")
      restartVisualization()
    }
  }, [isRecording, restartVisualization])

  useEffect(() => {
    document.addEventListener("visibilitychange", handleVisibilityChange)

    return () => {
      document.removeEventListener("visibilitychange", handleVisibilityChange)
    }
  }, [handleVisibilityChange])

  return (
    <Card className="w-full max-w-md mx-auto overflow-hidden bg-gradient-to-br from-white to-slate-50 dark:from-slate-900 dark:to-slate-800 shadow-md border-0">
      <CardContent className="p-5">
        <Tabs defaultValue="record" className="w-full">
          <TabsList className="grid w-full grid-cols-2 mb-6 bg-gradient-to-r from-slate-100 to-slate-200 dark:from-slate-800 dark:to-slate-700 p-1 rounded-lg shadow-inner">
            <TabsTrigger
              value="record"
              className="data-[state=active]:bg-white data-[state=active]:text-indigo-600 data-[state=active]:shadow-md dark:data-[state=active]:bg-slate-700 dark:data-[state=active]:text-indigo-300 transition-all"
            >
              Record
            </TabsTrigger>
            <TabsTrigger
              value="upload"
              className="data-[state=active]:bg-white data-[state=active]:text-indigo-600 data-[state=active]:shadow-md dark:data-[state=active]:bg-slate-700 dark:data-[state=active]:text-indigo-300 transition-all"
            >
              Upload
            </TabsTrigger>
          </TabsList>

          <TabsContent value="record" className="space-y-5">
            {/* Frequency Spectrum Analyzer - only show when not in post-recording mode */}
            {!isPostRecording && (
              <div className="space-y-2">
                <div className="flex items-center justify-between">
                  <span className="text-sm font-medium text-slate-700 dark:text-slate-300">Frequency Spectrum</span>
                  <button
                    onClick={toggleMute}
                    className={`p-1.5 rounded-full transition-all ${isMuted ? "bg-red-100 text-red-600 shadow-inner" : "hover:bg-indigo-100 hover:text-indigo-600 dark:hover:bg-slate-700"}`}
                    disabled={!isRecording}
                    title={isMuted ? "Unmute microphone" : "Mute microphone"}
                  >
                    {isMuted ? <VolumeX className="h-4 w-4" /> : <Volume2 className="h-4 w-4" />}
                  </button>
                </div>
                <div
                  className={`h-20 bg-gradient-to-b from-slate-900 to-black rounded-lg overflow-hidden relative flex items-end p-1 gap-[2px] shadow-lg ${isMuted ? "opacity-50" : ""}`}
                >
                  {analyserRef.current &&
                    (() => {
                      const bufferLength = analyserRef.current.frequencyBinCount
                      const dataArray = new Uint8Array(bufferLength)
                      analyserRef.current.getByteFrequencyData(dataArray)

                      // We'll display 32 frequency bands
                      const bands = 32
                      const barData = []

                      // Group frequency data into bands
                      for (let i = 0; i < bands; i++) {
                        // Logarithmic distribution gives better representation of how we hear
                        // Low frequencies get more detail than high frequencies
                        const startIndex = Math.floor((Math.pow(i / bands, 2) * bufferLength) / 4)
                        const endIndex = Math.floor((Math.pow((i + 1) / bands, 2) * bufferLength) / 4)

                        let sum = 0
                        for (let j = startIndex; j < endIndex; j++) {
                          sum += dataArray[j]
                        }

                        // Calculate average and apply gain reduction
                        const average = (sum / (endIndex - startIndex) || 0) * 0.6 // Reduce gain by 40%
                        barData.push(average)
                      }

                      return barData.map((value, i) => (
                        <div
                          key={i}
                          className="flex-1 transition-all duration-75 rounded-sm"
                          style={{
                            // Apply a threshold to reduce sensitivity to quiet sounds
                            height: `${Math.max(0, Math.min(100, value - 15))}%`, // Subtract a threshold of 15
                            backgroundColor: getBarColor(i, value),
                            boxShadow: value > 50 ? "0 0 8px rgba(255, 255, 255, 0.5)" : "none",
                            transform: `scaleY(${value > 15 ? 1 : 0.1})`, // Only fully show bars above threshold
                          }}
                        />
                      ))
                    })()}
                  <div className="absolute top-0 left-0 right-0 h-px bg-white/20" />
                  <div className="absolute top-1/4 left-0 right-0 h-px bg-white/20" />
                  <div className="absolute top-1/2 left-0 right-0 h-px bg-white/20" />
                  <div className="absolute top-3/4 left-0 right-0 h-px bg-white/20" />

                  {/* Frequency labels */}
                  <div className="absolute bottom-0 left-0 text-[6px] text-white/60 ml-1">20Hz</div>
                  <div className="absolute bottom-0 left-1/4 text-[6px] text-white/60">200Hz</div>
                  <div className="absolute bottom-0 left-1/2 text-[6px] text-white/60 -translate-x-1/2">2kHz</div>
                  <div className="absolute bottom-0 right-0 text-[6px] text-white/60 mr-1">20kHz</div>

                  {/* Muted overlay */}
                  {isMuted && (
                    <div className="absolute inset-0 flex items-center justify-center bg-black/30 backdrop-blur-sm">
                      <span className="text-white text-xs font-medium px-2 py-1 bg-red-600 rounded-sm shadow-lg">
                        MUTED
                      </span>
                    </div>
                  )}
                </div>
              </div>
            )}

            {/* Recording Time - show in both modes */}
            <div className="text-center text-3xl font-mono bg-gradient-to-r from-indigo-500 to-purple-600 bg-clip-text text-transparent font-bold">
              {formatTime(recordingTime)}
              {isMuted && <span className="text-xs text-red-500 ml-2 font-normal">(Muted)</span>}
            </div>

            {/* Recording Controls - only show when not in post-recording mode */}
            {!isPostRecording && (
              <div className="flex justify-center gap-4 my-6">
                {!isRecording ? (
                  <Button
                    onClick={startRecording}
                    variant="default"
                    size="icon"
                    className="h-16 w-16 rounded-full bg-gradient-to-r from-indigo-500 to-purple-600 hover:from-indigo-600 hover:to-purple-700 shadow-lg hover:shadow-xl transition-all duration-300 border-none"
                  >
                    <Mic className="h-7 w-7 text-white" />
                    <span className="sr-only">Start Recording</span>
                  </Button>
                ) : (
                  <Button
                    onClick={stopRecording}
                    variant="destructive"
                    size="icon"
                    className="h-16 w-16 rounded-full bg-gradient-to-r from-red-500 to-pink-600 hover:from-red-600 hover:to-pink-700 shadow-lg hover:shadow-xl transition-all duration-300 border-none"
                  >
                    <Square className="h-7 w-7 text-white" />
                    <span className="sr-only">Stop Recording</span>
                  </Button>
                )}
              </div>
            )}

            {/* Playback Controls and AI Processing (only shown when recording is available) */}
            {audioURL && (
              <div className="space-y-5 mt-4 pt-4 border-t border-slate-200 dark:border-slate-700">
                <div className="flex justify-center items-center">
                  <Button
                    onClick={togglePlayRecorded}
                    variant="outline"
                    size="icon"
                    className="bg-white dark:bg-slate-800 shadow-md hover:bg-slate-50 dark:hover:bg-slate-700 transition-all"
                  >
                    {isPlaying ? <Pause className="h-4 w-4" /> : <Play className="h-4 w-4" />}
                    <span className="sr-only">{isPlaying ? "Pause" : "Play"}</span>
                  </Button>

                  <Button
                    onClick={clearRecording}
                    variant="outline"
                    size="icon"
                    className="ml-3 bg-white dark:bg-slate-800 shadow-md hover:bg-slate-50 dark:hover:bg-slate-700 transition-all"
                  >
                    <Trash2 className="h-4 w-4" />
                    <span className="sr-only">Delete Recording</span>
                  </Button>

                  <Button
                    variant="outline"
                    size="icon"
                    className="ml-3 bg-white dark:bg-slate-800 shadow-md hover:bg-slate-50 dark:hover:bg-slate-700 transition-all"
                    onClick={() => {
                      if (audioURL) {
                        const a = document.createElement("a")
                        a.href = audioURL
                        a.download = `recording-${new Date().toISOString()}.webm`
                        a.click()
                      }
                    }}
                  >
                    <Save className="h-4 w-4" />
                    <span className="sr-only">Save Recording</span>
                  </Button>

                  {/* Vertical separator */}
                  <Separator orientation="vertical" className="mx-3 h-6" />

                  {/* New Recording button */}
                  <Button
                    onClick={startNewRecording}
                    variant="outline"
                    size="icon"
                    className="bg-white dark:bg-slate-800 shadow-md hover:bg-red-50 text-red-500 hover:text-red-600 dark:hover:bg-red-900/20 dark:text-red-400 dark:hover:text-red-300 border-red-200 dark:border-red-900/30 transition-all"
                  >
                    <PlusCircle className="h-4 w-4" />
                    <span className="sr-only">New Recording</span>
                  </Button>
                </div>

                {/* AI Processing Options with styled Select */}
                <div className="mt-5 space-y-4">
                  {/* Transcript Select Box */}
                  <div className="mb-3">
                    <Select defaultValue="collapsed">
                      <SelectTrigger className="w-full bg-white dark:bg-slate-800 shadow-md border-slate-200 dark:border-slate-700">
                        <SelectValue placeholder="View transcript" />
                      </SelectTrigger>
                      <SelectContent className="bg-white dark:bg-slate-800 border-slate-200 dark:border-slate-700 shadow-lg max-h-[300px] w-[var(--radix-select-trigger-width)]">
                        <div className="p-3 text-sm whitespace-pre-line text-slate-700 dark:text-slate-300 max-h-[250px] overflow-y-auto overflow-x-hidden break-words">
                          {transcriptContent}
                        </div>
                      </SelectContent>
                    </Select>
                  </div>

                  <div className="flex items-center gap-2">
                    <Select value={selectedAiAction} onValueChange={setSelectedAiAction}>
                      <SelectTrigger className="flex-1 bg-white dark:bg-slate-800 shadow-md border-slate-200 dark:border-slate-700">
                        <SelectValue placeholder="Select an AI action" />
                      </SelectTrigger>
                      <SelectContent className="bg-white dark:bg-slate-800 border-slate-200 dark:border-slate-700 shadow-lg">
                        <SelectItem value="transcribe">Transcribe audio to text</SelectItem>
                        <SelectItem value="summarize">Summarize content</SelectItem>
                        <SelectItem value="sentiment">Analyze sentiment</SelectItem>
                        <SelectItem value="translate">Translate to Spanish</SelectItem>
                        <SelectItem value="enhance">Enhance audio quality</SelectItem>
                      </SelectContent>
                    </Select>

                    <Button
                      onClick={processWithAI}
                      disabled={aiProcessing}
                      className="whitespace-nowrap bg-gradient-to-r from-indigo-500 to-purple-600 hover:from-indigo-600 hover:to-purple-700 shadow-md hover:shadow-lg transition-all duration-300 border-none"
                    >
                      {aiProcessing ? "Processing..." : "Do it..."}
                    </Button>
                  </div>

                  {aiResult && (
                    <div className="p-4 bg-gradient-to-br from-indigo-50 to-purple-50 dark:from-slate-800 dark:to-slate-700 rounded-lg text-sm shadow-md border border-indigo-100 dark:border-slate-600">
                      <p className="font-medium mb-2 text-indigo-700 dark:text-indigo-300">Result:</p>
                      <p className="whitespace-pre-line text-slate-700 dark:text-slate-300">{aiResult}</p>
                    </div>
                  )}
                </div>

                <audio ref={audioRef} className="hidden" />
              </div>
            )}
          </TabsContent>

          <TabsContent value="upload" className="space-y-4">
            <div className="border-2 border-dashed border-slate-200 dark:border-slate-700 rounded-lg p-8 text-center bg-gradient-to-br from-slate-50 to-white dark:from-slate-800 dark:to-slate-900 shadow-inner">
              <Upload className="h-10 w-10 mx-auto mb-3 text-indigo-400" />
              <p className="text-sm text-slate-500 dark:text-slate-400 mb-4">
                Drag and drop an audio file or click to browse
              </p>
              <input type="file" id="audio-upload" accept="audio/*" className="hidden" onChange={handleFileUpload} />
              <Button
                variant="outline"
                onClick={() => document.getElementById("audio-upload")?.click()}
                className="bg-white dark:bg-slate-800 shadow-md hover:bg-slate-50 dark:hover:bg-slate-700 transition-all"
              >
                Choose File
              </Button>
            </div>

            {uploadedFile && (
              <div className="space-y-3 p-4 bg-gradient-to-br from-indigo-50 to-purple-50 dark:from-slate-800 dark:to-slate-700 rounded-lg shadow-md">
                <p className="text-sm font-medium text-indigo-700 dark:text-indigo-300">{uploadedFile.name}</p>
                <div className="flex justify-center gap-4">
                  <Button
                    onClick={togglePlayUploaded}
                    variant="outline"
                    size="icon"
                    className="bg-white dark:bg-slate-800 shadow-md hover:bg-slate-50 dark:hover:bg-slate-700 transition-all"
                  >
                    {isPlaying ? <Pause className="h-4 w-4" /> : <Play className="h-4 w-4" />}
                    <span className="sr-only">{isPlaying ? "Pause" : "Play"}</span>
                  </Button>
                </div>
                <audio ref={audioRef} className="hidden" />
              </div>
            )}
          </TabsContent>
        </Tabs>
      </CardContent>
      <CardFooter className="flex justify-center bg-gradient-to-r from-indigo-50 to-purple-50 dark:from-slate-800 dark:to-slate-700 py-3">
        <p className="text-xs text-slate-500 dark:text-slate-400">
          {isRecording
            ? isMuted
              ? "Recording in progress (microphone muted)"
              : "Recording in progress..."
            : "Ready to record or upload audio"}
        </p>
      </CardFooter>
    </Card>
  )
}

